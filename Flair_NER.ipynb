{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682218542008,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"AZUd0T1MBTlA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56791,"status":"ok","timestamp":1682218598795,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"dZBlO6tMC3mg","outputId":"d82f21ec-adf0-4176-d480-5e548d6ef489"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":20065,"status":"ok","timestamp":1682218618856,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"oTbhgoB4frU0"},"outputs":[],"source":["# import json\n","# import csv\n","# import spacy\n","  \n","# #nlp = spacy.load('en_core_web_sm')\n","# #f= open('train_data_modified.csv', 'w', newline='',encoding='utf-8')\n","# #writer = csv.writer(f)\n","# #writer.writerow([\"sentence_no\",\"word\",\"pos_tag\",\"entitylabel\"])\n","# sent=1\n","# data=[]\n","# all_words=set()\n","\n","# with open(r\"C:\\Users\\Venkatesh Dharmaraj\\Downloads\\NLP Project - Ritika\\TBGA\\TBGA_train.txt\",'r') as f:\n","\n","#     for line in f:\n","#         d=json.loads(line)\n","#         sentence=[]\n","#         genestart=d['h']['pos'][0]\n","#         geneend=genestart+d['h']['pos'][1]\n","#         diseasestart=d['t']['pos'][0]\n","#         diseaseend=diseasestart+d['t']['pos'][1]\n","#         if genestart<=diseasestart:\n","\n","\n","#              sent_bef_e1=d['text'][:genestart-1]\n","#              for w in sent_bef_e1.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                all_words.add(w)\n","#                sentence.append(wt_pair)\n","#              #g_pair=[]\n","#              gene=d['text'][genestart:geneend]\n","#              gene_sub=gene.split()\n","#              if(len(gene_sub)==1):\n","#                all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                g_pair=(gene,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#              else:\n","#                gene_firstword=gene_sub[0]\n","#                all_words.add(gene_firstword)\n","#                g_pair=(gene_firstword,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#                for gw in gene_sub[1:]:\n","#                  all_words.add(gw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  g_pair=(gw,\"I-GENE\")\n","#                  sentence.append(g_pair)\n","\n","\n","             \n","#              sent_aft_e1=d['text'][geneend+1:diseasestart-1]\n","#              for w in sent_aft_e1.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                all_words.add(w)\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              #d_pair=[]\n","#              disease=d['text'][diseasestart:diseaseend]\n","#              '''\n","#              all_words.add(disease)\n","#              #d_pair.append(disease)\n","#              #d_pair.append(\"DISEASE\")\n","#              d_pair=(disease,\"DISEASE\")\n","#              sentence.append(d_pair)\n","#              '''\n","#              dis_sub=disease.split()\n","#              if(len(dis_sub)==1):\n","#                all_words.add(disease)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                d_pair=(disease,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#              else:\n","#                dis_firstword=dis_sub[0]\n","#                all_words.add(dis_firstword)\n","#                d_pair=(dis_firstword,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#                for dw in dis_sub[1:]:\n","#                  all_words.add(dw)\n","#              sent_aft_e2=d['text'][diseaseend+1:]\n","#              for w in sent_aft_e2.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                all_words.add(w)\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              data.append(sentence)\n","             \n","#         else:\n","           \n","#              sent_bef_e1=d['text'][:diseasestart-1]\n","#              for w in sent_bef_e1.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              #d_pair=[]\n","#              disease=d['text'][diseasestart:diseaseend]\n","#              '''\n","#              all_words.add(disease)\n","#              #d_pair.append(disease)\n","#              #d_pair.append(\"DISEASE\")\n","#              d_pair=(disease,\"DISEASE\")\n","#              sentence.append(d_pair)\n","#              '''\n","#              dis_sub=disease.split()\n","#              if(len(dis_sub)==1):\n","#                all_words.add(disease)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                d_pair=(disease,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#              else:\n","#                dis_firstword=dis_sub[0]\n","#                all_words.add(dis_firstword)\n","#                d_pair=(dis_firstword,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#                for dw in dis_sub[1:]:\n","#                  all_words.add(dw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  d_pair=(dw,\"I-DISEASE\")\n","#                  sentence.append(d_pair)\n","#              sent_aft_e1=d['text'][diseaseend+1:genestart-1]\n","#              for w in sent_aft_e1.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              #g_pair=[]\n","#              gene=d['text'][genestart:geneend]\n","#              gene_sub=gene.split()\n","#              if(len(gene_sub)==1):\n","#                all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                g_pair=(gene,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#              else:\n","#                gene_firstword=gene_sub[0]\n","#                all_words.add(gene_firstword)\n","#                g_pair=(gene_firstword,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#                for gw in gene_sub[1:]:\n","#                  all_words.add(gw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  g_pair=(gw,\"I-GENE\")\n","#                  sentence.append(g_pair)\n","#              '''\n","#              all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#              g_pair=(gene,\"GENE\")\n","#              sentence.append(g_pair)\n","#              '''\n","#              sent_aft_e2=d['text'][geneend+1:]\n","#              for w in sent_aft_e2.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              data.append(sentence)\n","\n","\n","\n","#         # print(d)\n","#         #writer.writerow([d['text'],d['h']['name'],d['t']['name'],d['relation']])"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1682218618857,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"LR46MwizDoWo","outputId":"366f4aac-25a3-4339-9341-f6eb93f29262"},"outputs":[],"source":["# data[0]"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1682218618858,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"oEPrDfSJDrXM","outputId":"d7aa5e4b-b51b-4ac6-cb98-12bbf51abcf4"},"outputs":[],"source":["# data[0]"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1682218618858,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"GuNvqtwMEe5J"},"outputs":[],"source":["# with open('readme.txt', 'w') as f:\n","#   f.write(\"hi\"+\"\\n\")\n","#   f.write(\"ji\")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4113,"status":"ok","timestamp":1682218622954,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"Ax-F_89sEC_R"},"outputs":[],"source":["# with open('train.txt', 'w',encoding=\"utf-8\") as f:\n","#     for d in data:\n","#       for tup in d:\n","#         f.write(tup[0]+\"\\t\"+tup[1]+\"\\n\")\n","#       f.write(\"\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1682218622955,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"vUKKsfoOFMLr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2268,"status":"ok","timestamp":1682218625209,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"sS3Zq26zNrEq"},"outputs":[],"source":["# import json\n","# import csv\n","# import spacy\n","  \n","# #nlp = spacy.load('en_core_web_sm')\n","# #f= open('train_data_modified.csv', 'w', newline='',encoding='utf-8')\n","# #writer = csv.writer(f)\n","# #writer.writerow([\"sentence_no\",\"word\",\"pos_tag\",\"entitylabel\"])\n","# sent=1\n","# data=[]\n","# all_words=set()\n","\n","# with open(r\"C:\\Users\\Venkatesh Dharmaraj\\Downloads\\NLP Project - Ritika\\TBGA\\TBGA_val.txt\",'r') as f:\n","#     for line in f:\n","#         d=json.loads(line)\n","#         sentence=[]\n","#         genestart=d['h']['pos'][0]\n","#         geneend=genestart+d['h']['pos'][1]\n","#         diseasestart=d['t']['pos'][0]\n","#         diseaseend=diseasestart+d['t']['pos'][1]\n","#         if genestart<=diseasestart:\n","\n","\n","#              sent_bef_e1=d['text'][:genestart-1]\n","#              for w in sent_bef_e1.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                all_words.add(w)\n","#                sentence.append(wt_pair)\n","#              #g_pair=[]\n","#              gene=d['text'][genestart:geneend]\n","#              gene_sub=gene.split()\n","#              if(len(gene_sub)==1):\n","#                all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                g_pair=(gene,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#              else:\n","#                gene_firstword=gene_sub[0]\n","#                all_words.add(gene_firstword)\n","#                g_pair=(gene_firstword,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#                for gw in gene_sub[1:]:\n","#                  all_words.add(gw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  g_pair=(gw,\"I-GENE\")\n","#                  sentence.append(g_pair)\n","\n","\n","             \n","#              sent_aft_e1=d['text'][geneend+1:diseasestart-1]\n","#              for w in sent_aft_e1.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                all_words.add(w)\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              #d_pair=[]\n","#              disease=d['text'][diseasestart:diseaseend]\n","#              '''\n","#              all_words.add(disease)\n","#              #d_pair.append(disease)\n","#              #d_pair.append(\"DISEASE\")\n","#              d_pair=(disease,\"DISEASE\")\n","#              sentence.append(d_pair)\n","#              '''\n","#              dis_sub=disease.split()\n","#              if(len(dis_sub)==1):\n","#                all_words.add(disease)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                d_pair=(disease,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#              else:\n","#                dis_firstword=dis_sub[0]\n","#                all_words.add(dis_firstword)\n","#                d_pair=(dis_firstword,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#                for dw in dis_sub[1:]:\n","#                  all_words.add(dw)\n","#              sent_aft_e2=d['text'][diseaseend+1:]\n","#              for w in sent_aft_e2.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                all_words.add(w)\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              data.append(sentence)\n","             \n","#         else:\n","           \n","#              sent_bef_e1=d['text'][:diseasestart-1]\n","#              for w in sent_bef_e1.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              #d_pair=[]\n","#              disease=d['text'][diseasestart:diseaseend]\n","#              '''\n","#              all_words.add(disease)\n","#              #d_pair.append(disease)\n","#              #d_pair.append(\"DISEASE\")\n","#              d_pair=(disease,\"DISEASE\")\n","#              sentence.append(d_pair)\n","#              '''\n","#              dis_sub=disease.split()\n","#              if(len(dis_sub)==1):\n","#                all_words.add(disease)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                d_pair=(disease,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#              else:\n","#                dis_firstword=dis_sub[0]\n","#                all_words.add(dis_firstword)\n","#                d_pair=(dis_firstword,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#                for dw in dis_sub[1:]:\n","#                  all_words.add(dw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  d_pair=(dw,\"I-DISEASE\")\n","#                  sentence.append(d_pair)\n","#              sent_aft_e1=d['text'][diseaseend+1:genestart-1]\n","#              for w in sent_aft_e1.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              #g_pair=[]\n","#              gene=d['text'][genestart:geneend]\n","#              gene_sub=gene.split()\n","#              if(len(gene_sub)==1):\n","#                all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                g_pair=(gene,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#              else:\n","#                gene_firstword=gene_sub[0]\n","#                all_words.add(gene_firstword)\n","#                g_pair=(gene_firstword,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#                for gw in gene_sub[1:]:\n","#                  all_words.add(gw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  g_pair=(gw,\"I-GENE\")\n","#                  sentence.append(g_pair)\n","#              '''\n","#              all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#              g_pair=(gene,\"GENE\")\n","#              sentence.append(g_pair)\n","#              '''\n","#              sent_aft_e2=d['text'][geneend+1:]\n","#              for w in sent_aft_e2.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              data.append(sentence)\n","\n","\n","\n","#         # print(d)\n","#         #writer.writerow([d['text'],d['h']['name'],d['t']['name'],d['relation']])"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2401,"status":"ok","timestamp":1682218627607,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"Xjirg4MDFUQv"},"outputs":[],"source":["# import json\n","# import csv\n","# import spacy\n","  \n","# #nlp = spacy.load('en_core_web_sm')\n","# #f= open('train_data_modified.csv', 'w', newline='',encoding='utf-8')\n","# #writer = csv.writer(f)\n","# #writer.writerow([\"sentence_no\",\"word\",\"pos_tag\",\"entitylabel\"])\n","# sent=1\n","# data=[]\n","# all_words=set()\n","\n","# with open(r\"C:\\Users\\Venkatesh Dharmaraj\\Downloads\\NLP Project - Ritika\\TBGA\\TBGA_val.txt\",'r') as f:\n","#     for line in f:\n","#         d=json.loads(line)\n","#         sentence=[]\n","#         genestart=d['h']['pos'][0]\n","#         geneend=genestart+d['h']['pos'][1]\n","#         diseasestart=d['t']['pos'][0]\n","#         diseaseend=diseasestart+d['t']['pos'][1]\n","#         if genestart<=diseasestart:\n","\n","\n","#              sent_bef_e1=d['text'][:genestart-1]\n","#              for w in sent_bef_e1.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"NORMAL\")\n","#                all_words.add(w)\n","#                sentence.append(wt_pair)\n","#              #g_pair=[]\n","#              gene=d['text'][genestart:geneend]\n","#              gene_sub=gene.split()\n","#              if(len(gene_sub)==1):\n","#                all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                g_pair=(gene,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#              else:\n","#                gene_firstword=gene_sub[0]\n","#                all_words.add(gene_firstword)\n","#                g_pair=(gene_firstword,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#                for gw in gene_sub[1:]:\n","#                  all_words.add(gw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  g_pair=(gw,\"I-GENE\")\n","#                  sentence.append(g_pair)\n","\n","\n","             \n","#              sent_aft_e1=d['text'][geneend+1:diseasestart-1]\n","#              for w in sent_aft_e1.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                all_words.add(w)\n","#                wt_pair=(w,\"NORMAL\")\n","#                sentence.append(wt_pair)\n","#              #d_pair=[]\n","#              disease=d['text'][diseasestart:diseaseend]\n","#              '''\n","#              all_words.add(disease)\n","#              #d_pair.append(disease)\n","#              #d_pair.append(\"DISEASE\")\n","#              d_pair=(disease,\"DISEASE\")\n","#              sentence.append(d_pair)\n","#              '''\n","#              dis_sub=disease.split()\n","#              if(len(dis_sub)==1):\n","#                all_words.add(disease)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                d_pair=(disease,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#              else:\n","#                dis_firstword=dis_sub[0]\n","#                all_words.add(dis_firstword)\n","#                d_pair=(dis_firstword,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#                for dw in dis_sub[1:]:\n","#                  all_words.add(dw)\n","#              sent_aft_e2=d['text'][diseaseend+1:]\n","#              for w in sent_aft_e2.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                all_words.add(w)\n","#                wt_pair=(w,\"NORMAL\")\n","#                sentence.append(wt_pair)\n","#              data.append(sentence)\n","             \n","#         else:\n","           \n","#              sent_bef_e1=d['text'][:diseasestart-1]\n","#              for w in sent_bef_e1.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"NORMAL\")\n","#                sentence.append(wt_pair)\n","#              #d_pair=[]\n","#              disease=d['text'][diseasestart:diseaseend]\n","#              '''\n","#              all_words.add(disease)\n","#              #d_pair.append(disease)\n","#              #d_pair.append(\"DISEASE\")\n","#              d_pair=(disease,\"DISEASE\")\n","#              sentence.append(d_pair)\n","#              '''\n","#              dis_sub=disease.split()\n","#              if(len(dis_sub)==1):\n","#                all_words.add(disease)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                d_pair=(disease,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#              else:\n","#                dis_firstword=dis_sub[0]\n","#                all_words.add(dis_firstword)\n","#                d_pair=(dis_firstword,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#                for dw in dis_sub[1:]:\n","#                  all_words.add(dw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  d_pair=(dw,\"I-DISEASE\")\n","#                  sentence.append(d_pair)\n","#              sent_aft_e1=d['text'][diseaseend+1:genestart-1]\n","#              for w in sent_aft_e1.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"NORMAL\")\n","#                sentence.append(wt_pair)\n","#              #g_pair=[]\n","#              gene=d['text'][genestart:geneend]\n","#              gene_sub=gene.split()\n","#              if(len(gene_sub)==1):\n","#                all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                g_pair=(gene,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#              else:\n","#                gene_firstword=gene_sub[0]\n","#                all_words.add(gene_firstword)\n","#                g_pair=(gene_firstword,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#                for gw in gene_sub[1:]:\n","#                  all_words.add(gw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  g_pair=(gw,\"I-GENE\")\n","#                  sentence.append(g_pair)\n","#              '''\n","#              all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#              g_pair=(gene,\"GENE\")\n","#              sentence.append(g_pair)\n","#              '''\n","#              sent_aft_e2=d['text'][geneend+1:]\n","#              for w in sent_aft_e2.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"NORMAL\")\n","#                sentence.append(wt_pair)\n","#              data.append(sentence)\n","\n","\n","\n","#         # print(d)\n","#         #writer.writerow([d['text'],d['h']['name'],d['t']['name'],d['relation']])"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":354,"status":"ok","timestamp":1682218627956,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"I5fZ-E31FYON"},"outputs":[],"source":["# with open('val.txt', 'w') as f:\n","#     for d in data:\n","#       for tup in d:\n","#         f.write(tup[0]+\"\\t\"+tup[1]+\"\\n\")\n","#       f.write(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682218627957,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"c_lUZMx6FdJV"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1754,"status":"ok","timestamp":1682218629709,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"uK92DSR1NxHX"},"outputs":[],"source":["# import json\n","# import csv\n","# import spacy\n","  \n","# #nlp = spacy.load('en_core_web_sm')\n","# #f= open('train_data_modified.csv', 'w', newline='',encoding='utf-8')\n","# #writer = csv.writer(f)\n","# #writer.writerow([\"sentence_no\",\"word\",\"pos_tag\",\"entitylabel\"])\n","# sent=1\n","# data=[]\n","# all_words=set()\n","\n","# with open(r\"C:\\Users\\Venkatesh Dharmaraj\\Downloads\\NLP Project - Ritika\\TBGA\\TBGA_test.txt\",'r') as f:\n","#     for line in f:\n","#         d=json.loads(line)\n","#         sentence=[]\n","#         genestart=d['h']['pos'][0]\n","#         geneend=genestart+d['h']['pos'][1]\n","#         diseasestart=d['t']['pos'][0]\n","#         diseaseend=diseasestart+d['t']['pos'][1]\n","#         if genestart<=diseasestart:\n","\n","\n","#              sent_bef_e1=d['text'][:genestart-1]\n","#              for w in sent_bef_e1.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                all_words.add(w)\n","#                sentence.append(wt_pair)\n","#              #g_pair=[]\n","#              gene=d['text'][genestart:geneend]\n","#              gene_sub=gene.split()\n","#              if(len(gene_sub)==1):\n","#                all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                g_pair=(gene,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#              else:\n","#                gene_firstword=gene_sub[0]\n","#                all_words.add(gene_firstword)\n","#                g_pair=(gene_firstword,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#                for gw in gene_sub[1:]:\n","#                  all_words.add(gw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  g_pair=(gw,\"I-GENE\")\n","#                  sentence.append(g_pair)\n","\n","\n","             \n","#              sent_aft_e1=d['text'][geneend+1:diseasestart-1]\n","#              for w in sent_aft_e1.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                all_words.add(w)\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              #d_pair=[]\n","#              disease=d['text'][diseasestart:diseaseend]\n","#              '''\n","#              all_words.add(disease)\n","#              #d_pair.append(disease)\n","#              #d_pair.append(\"DISEASE\")\n","#              d_pair=(disease,\"DISEASE\")\n","#              sentence.append(d_pair)\n","#              '''\n","#              dis_sub=disease.split()\n","#              if(len(dis_sub)==1):\n","#                all_words.add(disease)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                d_pair=(disease,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#              else:\n","#                dis_firstword=dis_sub[0]\n","#                all_words.add(dis_firstword)\n","#                d_pair=(dis_firstword,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#                for dw in dis_sub[1:]:\n","#                  all_words.add(dw)\n","#              sent_aft_e2=d['text'][diseaseend+1:]\n","#              for w in sent_aft_e2.split():\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                all_words.add(w)\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              data.append(sentence)\n","             \n","#         else:\n","           \n","#              sent_bef_e1=d['text'][:diseasestart-1]\n","#              for w in sent_bef_e1.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              #d_pair=[]\n","#              disease=d['text'][diseasestart:diseaseend]\n","#              '''\n","#              all_words.add(disease)\n","#              #d_pair.append(disease)\n","#              #d_pair.append(\"DISEASE\")\n","#              d_pair=(disease,\"DISEASE\")\n","#              sentence.append(d_pair)\n","#              '''\n","#              dis_sub=disease.split()\n","#              if(len(dis_sub)==1):\n","#                all_words.add(disease)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                d_pair=(disease,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#              else:\n","#                dis_firstword=dis_sub[0]\n","#                all_words.add(dis_firstword)\n","#                d_pair=(dis_firstword,\"B-DISEASE\")\n","#                sentence.append(d_pair)\n","#                for dw in dis_sub[1:]:\n","#                  all_words.add(dw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  d_pair=(dw,\"I-DISEASE\")\n","#                  sentence.append(d_pair)\n","#              sent_aft_e1=d['text'][diseaseend+1:genestart-1]\n","#              for w in sent_aft_e1.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              #g_pair=[]\n","#              gene=d['text'][genestart:geneend]\n","#              gene_sub=gene.split()\n","#              if(len(gene_sub)==1):\n","#                all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                g_pair=(gene,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#              else:\n","#                gene_firstword=gene_sub[0]\n","#                all_words.add(gene_firstword)\n","#                g_pair=(gene_firstword,\"B-GENE\")\n","#                sentence.append(g_pair)\n","#                for gw in gene_sub[1:]:\n","#                  all_words.add(gw)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#                  g_pair=(gw,\"I-GENE\")\n","#                  sentence.append(g_pair)\n","#              '''\n","#              all_words.add(gene)\n","#              #g_pair.append(gene)\n","#              #g_pair.append(\"GENE\")\n","#              g_pair=(gene,\"GENE\")\n","#              sentence.append(g_pair)\n","#              '''\n","#              sent_aft_e2=d['text'][geneend+1:]\n","#              for w in sent_aft_e2.split():\n","#                all_words.add(w)\n","#                #wt_pair=[]\n","#                #wt_pair.append(w)\n","#                #wt_pair.append(\"NORMAL\")\n","#                wt_pair=(w,\"O\")\n","#                sentence.append(wt_pair)\n","#              data.append(sentence)\n","\n","\n","\n","#         # print(d)\n","#         #writer.writerow([d['text'],d['h']['name'],d['t']['name'],d['relation']])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682218629710,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"OXDo0Db7FgHI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":493,"status":"ok","timestamp":1682218631638,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"hTWO4jGhFfhJ"},"outputs":[],"source":["# with open('test.txt', 'w') as f:\n","#     for d in data:\n","#       for tup in d:\n","#         f.write(tup[0]+\"\\t\"+tup[1]+\"\\n\")\n","#       f.write(\"\\n\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74745,"status":"ok","timestamp":1682218706379,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"QzGaaP2pHM4a","outputId":"b4a841bb-a262-4ada-8d90-fb1a8b87ad84"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: flair in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.12.2)\n","Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.2.2)\n","Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (2.0.0+cu117)\n","Requirement already satisfied: mpld3==0.3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.3)\n","Requirement already satisfied: wikipedia-api in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.5.8)\n","Requirement already satisfied: deprecated>=1.2.4 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.2.13)\n","Requirement already satisfied: boto3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.26.118)\n","Requirement already satisfied: huggingface-hub>=0.10.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.13.4)\n","Requirement already satisfied: bpemb>=0.3.2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.3.4)\n","Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.64.1)\n","Requirement already satisfied: ftfy in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (6.1.1)\n","Requirement already satisfied: lxml in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.9.2)\n","Requirement already satisfied: gensim>=3.8.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.3.0)\n","Requirement already satisfied: regex in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (2022.10.31)\n","Requirement already satisfied: pytorch-revgrad in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.2.0)\n","Requirement already satisfied: segtok>=1.5.7 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.5.11)\n","Requirement already satisfied: gdown==4.4.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.4.0)\n","Requirement already satisfied: pptree in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (3.1)\n","Requirement already satisfied: sqlitedict>=1.6.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (2.1.0)\n","Requirement already satisfied: conllu>=4.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.5.2)\n","Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (3.6.2)\n","Requirement already satisfied: janome in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.4.2)\n","Requirement already satisfied: tabulate in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.9.0)\n","Requirement already satisfied: transformer-smaller-training-vocab>=0.2.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.2.3)\n","Requirement already satisfied: transformers[sentencepiece]>=4.18.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.28.1)\n","Requirement already satisfied: more-itertools in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (9.1.0)\n","Requirement already satisfied: langdetect in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.0.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (2.8.2)\n","Requirement already satisfied: hyperopt>=0.2.7 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.2.7)\n","Requirement already satisfied: requests[socks] in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair) (2.28.2)\n","Requirement already satisfied: beautifulsoup4 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair) (4.11.1)\n","Requirement already satisfied: six in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair) (1.16.0)\n","Requirement already satisfied: filelock in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair) (3.10.4)\n","Requirement already satisfied: numpy in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.23.5)\n","Requirement already satisfied: sentencepiece in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bpemb>=0.3.2->flair) (0.1.98)\n","Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.15.0)\n","Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim>=3.8.0->flair) (2.0.5)\n","Requirement already satisfied: Cython==0.29.32 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim>=3.8.0->flair) (0.29.32)\n","Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim>=3.8.0->flair) (6.3.0)\n","Requirement already satisfied: scipy>=1.7.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim>=3.8.0->flair) (1.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (6.0)\n","Requirement already satisfied: py4j in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.10.9.7)\n","Requirement already satisfied: future in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.18.3)\n","Requirement already satisfied: cloudpickle in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.2.1)\n","Requirement already satisfied: networkx>=2.2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperopt>=0.2.7->flair) (3.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n","Requirement already satisfied: pillow>=6.2.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.3.0)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.38.0)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.0.6)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n","Requirement already satisfied: joblib>=1.1.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n","Requirement already satisfied: sympy in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (1.11.1)\n","Requirement already satisfied: jinja2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (3.1.2)\n","Requirement already satisfied: colorama in c:\\users\\venkatesh dharmaraj\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.26.0->flair) (0.4.6)\n","Requirement already satisfied: datasets<3.0.0,>=2.0.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformer-smaller-training-vocab>=0.2.1->flair) (2.11.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]>=4.18.0->flair) (0.13.3)\n","Requirement already satisfied: protobuf<=3.20.2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]>=4.18.0->flair) (3.19.6)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->flair) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->flair) (0.6.0)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.118 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->flair) (1.29.118)\n","Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\venkatesh dharmaraj\\appdata\\roaming\\python\\python310\\site-packages (from ftfy->flair) (0.2.6)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from botocore<1.30.0,>=1.29.118->boto3->flair) (1.26.14)\n","Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (11.0.0)\n","Requirement already satisfied: aiohttp in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (3.8.4)\n","Requirement already satisfied: multiprocess in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.70.14)\n","Requirement already satisfied: pandas in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.5.3)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (2023.4.0)\n","Requirement already satisfied: responses<0.19 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.18.0)\n","Requirement already satisfied: xxhash in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (3.2.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.3.6)\n","Requirement already satisfied: pyfume in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim>=3.8.0->flair) (0.2.25)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (3.0.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (3.4)\n","Requirement already satisfied: soupsieve>1.2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.2.post1)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch!=1.8,>=1.5.0->flair) (2.1.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (22.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (4.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.9.1)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (2022.7.1)\n","Requirement already satisfied: fst-pso in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=3.8.0->flair) (1.8.1)\n","Requirement already satisfied: simpful in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=3.8.0->flair) (2.10.0)\n","Requirement already satisfied: miniful in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim>=3.8.0->flair) (0.0.6)\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip available: 22.2.2 -> 23.1.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install flair"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15468,"status":"ok","timestamp":1682218721842,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"khlFgTzvHrLO","outputId":"b581dfa2-ebbc-469a-f808-0832372473b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: flair in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.12.2)"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip available: 22.2.2 -> 23.1.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["\n","Requirement already satisfied: gdown==4.4.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.4.0)\n","Requirement already satisfied: boto3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.26.118)\n","Requirement already satisfied: transformer-smaller-training-vocab>=0.2.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.2.3)\n","Requirement already satisfied: hyperopt>=0.2.7 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.2.7)\n","Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (2.8.2)\n","Requirement already satisfied: transformers[sentencepiece]>=4.18.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.28.1)\n","Requirement already satisfied: pytorch-revgrad in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.2.0)\n","Requirement already satisfied: lxml in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.9.2)\n","Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.2.2)\n","Requirement already satisfied: gensim>=3.8.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.3.0)\n","Requirement already satisfied: huggingface-hub>=0.10.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.13.4)\n","Requirement already satisfied: conllu>=4.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.5.2)\n","Requirement already satisfied: tabulate in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.9.0)\n","Requirement already satisfied: regex in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (2022.10.31)\n","Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (2.0.0+cu117)\n","Requirement already satisfied: deprecated>=1.2.4 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.2.13)\n","Requirement already satisfied: ftfy in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (6.1.1)\n","Requirement already satisfied: segtok>=1.5.7 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.5.11)\n","Requirement already satisfied: langdetect in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (1.0.9)\n","Requirement already satisfied: bpemb>=0.3.2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.3.4)\n","Requirement already satisfied: mpld3==0.3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.3)\n","Requirement already satisfied: janome in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.4.2)\n","Requirement already satisfied: sqlitedict>=1.6.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (2.1.0)\n","Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (3.6.2)\n","Requirement already satisfied: wikipedia-api in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (0.5.8)\n","Requirement already satisfied: pptree in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (3.1)\n","Requirement already satisfied: more-itertools in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (9.1.0)\n","Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from flair) (4.64.1)\n","Requirement already satisfied: requests[socks] in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair) (2.28.2)\n","Requirement already satisfied: beautifulsoup4 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair) (4.11.1)\n","Requirement already satisfied: six in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair) (1.16.0)\n","Requirement already satisfied: filelock in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown==4.4.0->flair) (3.10.4)\n","Requirement already satisfied: numpy in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.23.5)\n","Requirement already satisfied: sentencepiece in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bpemb>=0.3.2->flair) (0.1.98)\n","Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.15.0)\n","Requirement already satisfied: scipy>=1.7.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim>=3.8.0->flair) (1.10.0)\n","Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim>=3.8.0->flair) (6.3.0)\n","Requirement already satisfied: Cython==0.29.32 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim>=3.8.0->flair) (0.29.32)\n","Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim>=3.8.0->flair) (2.0.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (6.0)\n","Requirement already satisfied: cloudpickle in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.2.1)\n","Requirement already satisfied: future in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.18.3)\n","Requirement already satisfied: py4j in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.10.9.7)\n","Requirement already satisfied: networkx>=2.2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperopt>=0.2.7->flair) (3.0)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.38.0)\n","Requirement already satisfied: pillow>=6.2.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.0.6)\n","Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.2.0)\n","Requirement already satisfied: sympy in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (1.11.1)\n","Requirement already satisfied: jinja2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (3.1.2)\n","Requirement already satisfied: colorama in c:\\users\\venkatesh dharmaraj\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.26.0->flair) (0.4.6)\n","Requirement already satisfied: datasets<3.0.0,>=2.0.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformer-smaller-training-vocab>=0.2.1->flair) (2.11.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]>=4.18.0->flair) (0.13.3)\n","Requirement already satisfied: protobuf<=3.20.2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]>=4.18.0->flair) (3.19.6)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.118 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->flair) (1.29.118)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->flair) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->flair) (0.6.0)\n","Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\venkatesh dharmaraj\\appdata\\roaming\\python\\python310\\site-packages (from ftfy->flair) (0.2.6)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from botocore<1.30.0,>=1.29.118->boto3->flair) (1.26.14)\n","Requirement already satisfied: xxhash in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (3.2.0)\n","Requirement already satisfied: pandas in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.5.3)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.3.6)\n","Requirement already satisfied: multiprocess in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (2023.4.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (11.0.0)\n","Requirement already satisfied: responses<0.19 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (0.18.0)\n","Requirement already satisfied: aiohttp in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (3.8.4)\n","Requirement already satisfied: pyfume in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim>=3.8.0->flair) (0.2.25)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (3.0.1)\n","Requirement already satisfied: soupsieve>1.2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.2.post1)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch!=1.8,>=1.5.0->flair) (2.1.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.3.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (4.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.9.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (22.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (1.3.3)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair) (2022.7.1)\n","Requirement already satisfied: fst-pso in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=3.8.0->flair) (1.8.1)\n","Requirement already satisfied: simpful in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=3.8.0->flair) (2.10.0)\n","Requirement already satisfied: miniful in c:\\users\\venkatesh dharmaraj\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim>=3.8.0->flair) (0.0.6)\n"]}],"source":["!pip install --user flair\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149431,"status":"ok","timestamp":1682218871269,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"O6vmRcX5FnCO","outputId":"6df3773a-e83d-4cf8-9869-3204c85ffa6f"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["2023-04-23 15:22:17,037 Reading data from C:\\Users\\Venkatesh Dharmaraj\\Downloads\\NLP Project - Ritika\\TGBA-Formatted\n","2023-04-23 15:22:17,039 Train: C:\\Users\\Venkatesh Dharmaraj\\Downloads\\NLP Project - Ritika\\TGBA-Formatted\\train.txt\n","2023-04-23 15:22:17,040 Dev: C:\\Users\\Venkatesh Dharmaraj\\Downloads\\NLP Project - Ritika\\TGBA-Formatted\\val.txt\n","2023-04-23 15:22:17,040 Test: C:\\Users\\Venkatesh Dharmaraj\\Downloads\\NLP Project - Ritika\\TGBA-Formatted\\test.txt\n"]}],"source":["from flair.data import Corpus\n","from flair.datasets import ColumnCorpus\n","# define columns\n","columns = {0 : 'text', 1 : 'ner'}\n","# directory where the data resides\n","data_folder = r'C:\\Users\\Venkatesh Dharmaraj\\Downloads\\NLP Project - Ritika\\TGBA-Formatted'\n","# initializing the corpus\n","corpus: Corpus = ColumnCorpus(data_folder, columns,\n","                              train_file = 'train.txt',\n","                              test_file = 'test.txt',\n","                              dev_file = 'val.txt')"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12420,"status":"ok","timestamp":1682218883666,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"r4atxC60HVcz","outputId":"d137eab4-8d87-4c33-cb0e-529eb2c30810"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-04-23 15:23:44,477 Computing label dictionary. Progress:\n"]},{"name":"stderr","output_type":"stream","text":["178264it [00:03, 49033.59it/s]"]},{"name":"stdout","output_type":"stream","text":["2023-04-23 15:23:48,198 Dictionary created for label 'ner' with 3 values: DISEASE (seen 178264 times), GENE (seen 178212 times)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["\n","tag_type = 'ner'\n","# make tag dictionary from the corpus\n","tag_dict = corpus.make_label_dictionary(tag_type)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1682218883666,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"g5GijykIHx3k","outputId":"4c4f09a1-bf22-4690-bebc-61f6af347f76"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dictionary with 3 tags: <unk>, DISEASE, GENE\n"]}],"source":["print(tag_dict)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19248,"status":"ok","timestamp":1682218902900,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"ek5HljN2HzjZ","outputId":"406792fd-4b67-4333-de71-0f03c454e996"},"outputs":[],"source":["from flair.embeddings import WordEmbeddings, StackedEmbeddings\n","from typing import List\n","#embedding_types : List[TokenEmbeddings] = [\n"," #       WordEmbeddings('glove'),\n","  #      ## other embeddings\n","   #     ]\n","from flair.embeddings import TransformerWordEmbeddings\n","embeddings : TransformerWordEmbeddings = TransformerWordEmbeddings('bert-base-cased')\n","# glove_embedding: WordEmbeddings = WordEmbeddings('glove')\n","# embeddings : StackedEmbeddings = StackedEmbeddings(\n","#                                  embeddings=[glove_embedding])"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1682218902902,"user":{"displayName":"Nandila Bhattacharjee","userId":"09887993730391794279"},"user_tz":420},"id":"kPZ2w4feH4E9","outputId":"27add5cc-73fc-49f0-c6ad-c91566432cff"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-04-23 15:25:08,165 SequenceTagger predicts: Dictionary with 9 tags: O, S-DISEASE, B-DISEASE, E-DISEASE, I-DISEASE, S-GENE, B-GENE, E-GENE, I-GENE\n","SequenceTagger(\n","  (embeddings): TransformerWordEmbeddings(\n","    (model): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(28997, 768)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","  )\n","  (word_dropout): WordDropout(p=0.05)\n","  (locked_dropout): LockedDropout(p=0.5)\n","  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)\n","  (rnn): LSTM(768, 256, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=11, bias=True)\n","  (loss_function): ViterbiLoss()\n","  (crf): CRF()\n",")\n"]}],"source":["from flair.models import SequenceTagger\n","tagger : SequenceTagger = SequenceTagger(hidden_size=256,\n","                                       embeddings=embeddings,\n","                                       tag_dictionary=tag_dict,\n","                                       tag_type=tag_type,\n","                                       use_crf=True)\n","print(tagger)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JcVGW_5KIU2g","outputId":"ce4d7e5f-3af8-4f0a-f80a-02e4c211a147"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-04-23 15:33:30,195 ----------------------------------------------------------------------------------------------------\n","2023-04-23 15:33:30,212 Model: \"SequenceTagger(\n","  (embeddings): TransformerWordEmbeddings(\n","    (model): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(28997, 768)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","  )\n","  (word_dropout): WordDropout(p=0.05)\n","  (locked_dropout): LockedDropout(p=0.5)\n","  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)\n","  (rnn): LSTM(768, 256, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=11, bias=True)\n","  (loss_function): ViterbiLoss()\n","  (crf): CRF()\n",")\"\n","2023-04-23 15:33:30,213 ----------------------------------------------------------------------------------------------------\n","2023-04-23 15:33:30,214 Corpus: \"Corpus: 178264 train + 20193 dev + 20516 test sentences\"\n","2023-04-23 15:33:30,215 ----------------------------------------------------------------------------------------------------\n","2023-04-23 15:33:30,216 Parameters:\n","2023-04-23 15:33:30,217  - learning_rate: \"0.100000\"\n","2023-04-23 15:33:30,218  - mini_batch_size: \"16\"\n","2023-04-23 15:33:30,219  - patience: \"3\"\n","2023-04-23 15:33:30,219  - anneal_factor: \"0.5\"\n","2023-04-23 15:33:30,219  - max_epochs: \"1\"\n","2023-04-23 15:33:30,221  - shuffle: \"True\"\n","2023-04-23 15:33:30,221  - train_with_dev: \"False\"\n","2023-04-23 15:33:30,221  - batch_growth_annealing: \"False\"\n","2023-04-23 15:33:30,222 ----------------------------------------------------------------------------------------------------\n","2023-04-23 15:33:30,223 Model training base path: \"\\content\\drive\\MyDrive\\resources\\taggers\\bioner\"\n","2023-04-23 15:33:30,224 ----------------------------------------------------------------------------------------------------\n","2023-04-23 15:33:30,226 Device: cuda:0\n","2023-04-23 15:33:30,227 ----------------------------------------------------------------------------------------------------\n","2023-04-23 15:33:30,227 Embeddings storage mode: cpu\n","2023-04-23 15:33:30,228 ----------------------------------------------------------------------------------------------------\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.35 GiB already allocated; 4.50 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[1;32mIn[26], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# trainer : ModelTrainer = ModelTrainer(tagger, corpus, optimizer=optimizer)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m trainer : ModelTrainer \u001b[39m=\u001b[39m ModelTrainer(tagger, corpus)\n\u001b[1;32m----> 9\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m'\u001b[39;49m\u001b[39m/content/drive/MyDrive/resources/taggers/bioner\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m               learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m               mini_batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[0;32m     12\u001b[0m               max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     13\u001b[0m             )\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\trainers\\trainer.py:549\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self, base_path, learning_rate, mini_batch_size, eval_batch_size, mini_batch_chunk_size, max_epochs, train_with_dev, train_with_test, monitor_train, monitor_test, main_evaluation_metric, scheduler, anneal_factor, patience, min_learning_rate, initial_extra_patience, optimizer, cycle_momentum, warmup_fraction, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, anneal_with_prestarts, anneal_against_dev_loss, batch_growth_annealing, shuffle, param_selection_mode, write_weights, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, save_model_each_k_epochs, tensorboard_comment, use_swa, use_final_model_for_eval, gold_label_dictionary_for_eval, exclude_labels, create_file_logs, create_loss_file, epoch, use_tensorboard, tensorboard_log_dir, metrics_for_tensorboard, optimizer_state_dict, scheduler_state_dict, save_optimizer_state, reduce_transformer_vocab, shuffle_first_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[39m# forward and backward for batch\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[39mfor\u001b[39;00m batch_step \u001b[39min\u001b[39;00m batch_steps:\n\u001b[0;32m    548\u001b[0m     \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m--> 549\u001b[0m     loss, datapoint_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward_loss(batch_step)\n\u001b[0;32m    550\u001b[0m     average_over \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m datapoint_count\n\u001b[0;32m    551\u001b[0m     \u001b[39m# Backward\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py:274\u001b[0m, in \u001b[0;36mSequenceTagger.forward_loss\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    272\u001b[0m sentences \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(sentences, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m, reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    273\u001b[0m gold_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_label_tensor(sentences)\n\u001b[1;32m--> 274\u001b[0m sentence_tensor, lengths \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_tensors(sentences)\n\u001b[0;32m    276\u001b[0m \u001b[39m# forward pass to get scores\u001b[39;00m\n\u001b[0;32m    277\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(sentence_tensor, lengths)\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py:287\u001b[0m, in \u001b[0;36mSequenceTagger._prepare_tensors\u001b[1;34m(self, data_points)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     sentences \u001b[39m=\u001b[39m data_points\n\u001b[1;32m--> 287\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings\u001b[39m.\u001b[39;49membed(sentences)\n\u001b[0;32m    289\u001b[0m \u001b[39m# make a zero-padded tensor for the whole sentence\u001b[39;00m\n\u001b[0;32m    290\u001b[0m lengths, sentence_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_padded_tensor_for_batch(sentences)\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\base.py:49\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[1;34m(self, data_points)\u001b[0m\n\u001b[0;32m     46\u001b[0m     data_points \u001b[39m=\u001b[39m [data_points]\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_everything_embedded(data_points):\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_embeddings_internal(data_points)\n\u001b[0;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m data_points\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\transformer.py:662\u001b[0m, in \u001b[0;36mTransformerBaseEmbeddings._add_embeddings_internal\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    660\u001b[0m gradient_context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39menable_grad() \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfine_tune \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad()\n\u001b[0;32m    661\u001b[0m \u001b[39mwith\u001b[39;00m gradient_context:\n\u001b[1;32m--> 662\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_tensors(tensors)\n\u001b[0;32m    664\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_embedding:\n\u001b[0;32m    665\u001b[0m     document_embedding \u001b[39m=\u001b[39m embeddings[\u001b[39m\"\u001b[39m\u001b[39mdocument_embeddings\u001b[39m\u001b[39m\"\u001b[39m]\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\transformer.py:1319\u001b[0m, in \u001b[0;36mTransformerEmbeddings._forward_tensors\u001b[1;34m(self, tensors)\u001b[0m\n\u001b[0;32m   1318\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_tensors\u001b[39m(\u001b[39mself\u001b[39m, tensors) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m-> 1319\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtensors)\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\embeddings\\transformer.py:1223\u001b[0m, in \u001b[0;36mTransformerEmbeddings.forward\u001b[1;34m(self, input_ids, sub_token_lengths, token_lengths, attention_mask, overflow_to_sample_mapping, word_ids, langs, bbox, pixel_values)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[39mif\u001b[39;00m pixel_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1222\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pixel_values\n\u001b[1;32m-> 1223\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m   1224\u001b[0m \u001b[39m# make the tuple a tensor; makes working with it easier.\u001b[39;00m\n\u001b[0;32m   1225\u001b[0m hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(hidden_states)\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1016\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1017\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[0;32m   1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:230\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    227\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[0;32m    231\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    233\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n","\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.35 GiB already allocated; 4.50 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["from flair.trainers import ModelTrainer\n","from torch.optim import Adam\n","\n","parameters = tagger.parameters()\n","\n","optimizer = Adam(parameters, lr=0.1)\n","# trainer : ModelTrainer = ModelTrainer(tagger, corpus, optimizer=optimizer)\n","trainer : ModelTrainer = ModelTrainer(tagger, corpus)\n","trainer.train('/content/drive/MyDrive/resources/taggers/bioner',\n","              learning_rate=0.1,\n","              mini_batch_size=16,\n","              max_epochs=1,\n","            )"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["import torch\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading ()lve/main/config.json: 100%|| 483/483 [00:00<00:00, 138kB/s]\n","Downloading pytorch_model.bin: 100%|| 268M/268M [00:21<00:00, 12.6MB/s] \n","Downloading ()okenizer_config.json: 100%|| 28.0/28.0 [00:00<00:00, 9.41kB/s]\n","Downloading ()solve/main/vocab.txt: 100%|| 232k/232k [00:00<00:00, 1.88MB/s]\n","Downloading ()/main/tokenizer.json: 100%|| 466k/466k [00:00<00:00, 781kB/s]\n"]}],"source":["import transformers\n","transformer_model = transformers.AutoModel.from_pretrained('distilbert-base-uncased')\n","tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-04-23 15:39:38,913 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 4.00 GiB total capacity; 2.36 GiB already allocated; 4.50 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tagger \u001b[39m=\u001b[39m SequenceTagger\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mner\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# load the pre-trained NER model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tagger \u001b[39m=\u001b[39m SequenceTagger(hidden_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, \n\u001b[0;32m      3\u001b[0m                         embeddings_storage_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m                         tag_dictionary\u001b[39m=\u001b[39mtagger\u001b[39m.\u001b[39mtag_dictionary,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m                         transformer_model\u001b[39m=\u001b[39mtransformer_model,\n\u001b[0;32m      9\u001b[0m                         transformer_tokenizer\u001b[39m=\u001b[39mtokenizer)\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py:1035\u001b[0m, in \u001b[0;36mSequenceTagger.load\u001b[1;34m(cls, model_path)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1032\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mcls\u001b[39m, model_path: Union[\u001b[39mstr\u001b[39m, Path, Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSequenceTagger\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1033\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m cast\n\u001b[1;32m-> 1035\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\u001b[39m\"\u001b[39m\u001b[39mSequenceTagger\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mload(model_path\u001b[39m=\u001b[39;49mmodel_path))\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\nn\\model.py:559\u001b[0m, in \u001b[0;36mClassifier.load\u001b[1;34m(cls, model_path)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    556\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mcls\u001b[39m, model_path: Union[\u001b[39mstr\u001b[39m, Path, Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mClassifier\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    557\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m cast\n\u001b[1;32m--> 559\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\u001b[39m\"\u001b[39m\u001b[39mClassifier\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mload(model_path\u001b[39m=\u001b[39;49mmodel_path))\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\nn\\model.py:198\u001b[0m, in \u001b[0;36mModel.load\u001b[1;34m(cls, model_path)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m__cls__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m state:\n\u001b[0;32m    196\u001b[0m     state\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m__cls__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 198\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_init_model_with_state_dict(state)\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_card\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m state:\n\u001b[0;32m    201\u001b[0m     model\u001b[39m.\u001b[39mmodel_card \u001b[39m=\u001b[39m state[\u001b[39m\"\u001b[39m\u001b[39mmodel_card\u001b[39m\u001b[39m\"\u001b[39m]\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py:617\u001b[0m, in \u001b[0;36mSequenceTagger._init_model_with_state_dict\u001b[1;34m(cls, state, **kwargs)\u001b[0m\n\u001b[0;32m    614\u001b[0m         state[\u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcrf.transitions\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m state[\u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtransitions\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    615\u001b[0m         \u001b[39mdel\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtransitions\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 617\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_init_model_with_state_dict(\n\u001b[0;32m    618\u001b[0m     state,\n\u001b[0;32m    619\u001b[0m     embeddings\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    620\u001b[0m     tag_dictionary\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtag_dictionary\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    621\u001b[0m     tag_format\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtag_format\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBIOES\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    622\u001b[0m     tag_type\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtag_type\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    623\u001b[0m     use_crf\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muse_crf\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    624\u001b[0m     use_rnn\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muse_rnn\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    625\u001b[0m     rnn_layers\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrnn_layers\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    626\u001b[0m     hidden_size\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhidden_size\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    627\u001b[0m     dropout\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muse_dropout\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0.0\u001b[39m),\n\u001b[0;32m    628\u001b[0m     word_dropout\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muse_word_dropout\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0.0\u001b[39m),\n\u001b[0;32m    629\u001b[0m     locked_dropout\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muse_locked_dropout\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0.0\u001b[39m),\n\u001b[0;32m    630\u001b[0m     rnn_type\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrnn_type\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLSTM\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    631\u001b[0m     reproject_embeddings\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreproject_embeddings\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m    632\u001b[0m     loss_weights\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mweight_dict\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    633\u001b[0m     init_from_state_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    634\u001b[0m     train_initial_hidden_state\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtrain_initial_hidden_state\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m    635\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    636\u001b[0m )\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\nn\\model.py:89\u001b[0m, in \u001b[0;36mModel._init_model_with_state_dict\u001b[1;34m(cls, state, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m         embeddings \u001b[39m=\u001b[39m load_embeddings(embeddings)\n\u001b[0;32m     87\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m embeddings\n\u001b[1;32m---> 89\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     91\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(state[\u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m model\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py:202\u001b[0m, in \u001b[0;36mSequenceTagger.__init__\u001b[1;34m(self, embeddings, tag_dictionary, tag_type, use_rnn, rnn, rnn_type, tag_format, hidden_size, rnn_layers, bidirectional, use_crf, reproject_embeddings, dropout, word_dropout, locked_dropout, train_initial_hidden_state, loss_weights, init_from_state_dict, allow_unk_predictions)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrf \u001b[39m=\u001b[39m CRF(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_dictionary, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagset_size, init_from_state_dict)\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviterbi_decoder \u001b[39m=\u001b[39m ViterbiDecoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_dictionary)\n\u001b[1;32m--> 202\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto(flair\u001b[39m.\u001b[39;49mdevice)\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\models\\language_model.py:476\u001b[0m, in \u001b[0;36mLanguageModel._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    472\u001b[0m             _flat_weights_names\u001b[39m.\u001b[39mextend(param_names)\n\u001b[0;32m    474\u001b[0m     \u001b[39msetattr\u001b[39m(child_module, \u001b[39m\"\u001b[39m\u001b[39m_flat_weights_names\u001b[39m\u001b[39m\"\u001b[39m, _flat_weights_names)\n\u001b[1;32m--> 476\u001b[0m child_module\u001b[39m.\u001b[39;49m_apply(fn)\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:197\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m--> 197\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    199\u001b[0m     \u001b[39m# Resets _flat_weights\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[39m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[39m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_flat_weights()\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n","\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 4.00 GiB total capacity; 2.36 GiB already allocated; 4.50 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["tagger = SequenceTagger.load('ner') # load the pre-trained NER model\n","tagger = SequenceTagger(hidden_size=256, \n","                        embeddings_storage_mode='cpu',\n","                        tag_dictionary=tagger.tag_dictionary,\n","                        tag_type=tagger.tag_type,\n","                        use_crf=True,\n","                        use_transformer=True,\n","                        transformer_model=transformer_model,\n","                        transformer_tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer : ModelTrainer = ModelTrainer(tagger, corpus)\n","trainer.train('/content/drive/MyDrive/resources/taggers/bioner',\n","              learning_rate=0.1,\n","              mini_batch_size=16,\n","              max_epochs=1,\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1i_gGafeJ2HX"},"outputs":[],"source":["tagger.save('nlp_flair')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QILZPUcSUCXf"},"outputs":[{"ename":"HFValidationError","evalue":"Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/resources/tagger/bio-ner/final-model.pt'. Use `repo_type` argument if needed.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m SequenceTagger\n\u001b[0;32m      3\u001b[0m \u001b[39m# load the trained model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m SequenceTagger\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m/content/drive/MyDrive/resources/tagger/bio-ner/final-model.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# create example sentence\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# sentence = Sentence('I love Berlin')\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# # predict the tags\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# model.predict(sentence)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# print(sentence.to_tagged_string())\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py:1035\u001b[0m, in \u001b[0;36mSequenceTagger.load\u001b[1;34m(cls, model_path)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1032\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mcls\u001b[39m, model_path: Union[\u001b[39mstr\u001b[39m, Path, Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSequenceTagger\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1033\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m cast\n\u001b[1;32m-> 1035\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\u001b[39m\"\u001b[39m\u001b[39mSequenceTagger\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mload(model_path\u001b[39m=\u001b[39;49mmodel_path))\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\nn\\model.py:559\u001b[0m, in \u001b[0;36mClassifier.load\u001b[1;34m(cls, model_path)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    556\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mcls\u001b[39m, model_path: Union[\u001b[39mstr\u001b[39m, Path, Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mClassifier\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    557\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m cast\n\u001b[1;32m--> 559\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\u001b[39m\"\u001b[39m\u001b[39mClassifier\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mload(model_path\u001b[39m=\u001b[39;49mmodel_path))\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\nn\\model.py:190\u001b[0m, in \u001b[0;36mModel.load\u001b[1;34m(cls, model_path)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     \u001b[39m# if this class is not abstract, fetch the model and load it\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(model_path, \u001b[39mdict\u001b[39m):\n\u001b[1;32m--> 190\u001b[0m         model_file \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_model(\u001b[39mstr\u001b[39;49m(model_path))\n\u001b[0;32m    191\u001b[0m         state \u001b[39m=\u001b[39m load_torch_state(model_file)\n\u001b[0;32m    192\u001b[0m     \u001b[39melse\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py:858\u001b[0m, in \u001b[0;36mSequenceTagger._fetch_model\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfile_download\u001b[39;00m \u001b[39mimport\u001b[39;00m hf_hub_download\n\u001b[0;32m    857\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 858\u001b[0m     model_path \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    859\u001b[0m         repo_id\u001b[39m=\u001b[39;49mmodel_name,\n\u001b[0;32m    860\u001b[0m         filename\u001b[39m=\u001b[39;49mhf_model_name,\n\u001b[0;32m    861\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    862\u001b[0m         library_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mflair\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    863\u001b[0m         library_version\u001b[39m=\u001b[39;49mflair\u001b[39m.\u001b[39;49m__version__,\n\u001b[0;32m    864\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mflair\u001b[39m.\u001b[39;49mcache_root \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmodels\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m/\u001b[39;49m model_folder,\n\u001b[0;32m    865\u001b[0m     )\n\u001b[0;32m    866\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError:\n\u001b[0;32m    867\u001b[0m     \u001b[39m# output information\u001b[39;00m\n\u001b[0;32m    868\u001b[0m     log\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m80\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:112\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mfor\u001b[39;00m arg_name, arg_value \u001b[39min\u001b[39;00m chain(\n\u001b[0;32m    108\u001b[0m     \u001b[39mzip\u001b[39m(signature\u001b[39m.\u001b[39mparameters, args),  \u001b[39m# Args values\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     kwargs\u001b[39m.\u001b[39mitems(),  \u001b[39m# Kwargs values\u001b[39;00m\n\u001b[0;32m    110\u001b[0m ):\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m--> 112\u001b[0m         validate_repo_id(arg_value)\n\u001b[0;32m    114\u001b[0m     \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         has_token \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRepo id must be a string, not \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(repo_id)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[0;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    167\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n","\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/resources/tagger/bio-ner/final-model.pt'. Use `repo_type` argument if needed."]}],"source":["from flair.data import Sentence\n","from flair.models import SequenceTagger\n","# load the trained model\n","model = SequenceTagger.load('/content/drive/MyDrive/resources/tagger/bio-ner/final-model.pt')\n","# create example sentence\n","# sentence = Sentence('I love Berlin')\n","# # predict the tags\n","# model.predict(sentence)\n","# print(sentence.to_tagged_string())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXwHIn00UCJ5"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}
