{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Venkatesh Dharmaraj\\Downloads\\NLP Project\\data.csv\",na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_csv(r\"C:\\Users\\Venkatesh Dharmaraj\\Downloads\\NLP Project\\test_data.csv\",na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178264, 6)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_representation1(df):\n",
    "    for i,row in df.iterrows():\n",
    "        gs,ge=map(int,row['gene_index'][1:-1].split(','))\n",
    "        ds,dt=map(int,row['disease_index'][1:-1].split(','))\n",
    "        row['sentence'] = row['sentence'][:gs]+'@ * gene * '+row['sentence'][gs:gs+ge]+' @'+row['sentence'][gs+ge:]\n",
    "        row['sentence'] = row['sentence'][:ds]+'# ^ disease ^ '+row['sentence'][ds:ds+dt]+' #'+row['sentence'][ds+dt:]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=entity_representation1(df)\n",
    "test_df=entity_representation1(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In addition, the combined # ^ disease ^ cancer # genome expression metaanalysis datasets included @ * gene * PDE11A @ among the top 1% down-regulated genes in PCa.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Sentence'] = df['sentence']+df['gene']+df['disease']\n",
    "# df=df[df.columns[[3,4]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df.columns[[0,3]]]\n",
    "df= df.rename({'relation': 'Relation','sentence':'Sentence'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=test_df[test_df.columns[[0,3]]]\n",
    "test_df= test_df.rename({'relation': 'Relation','sentence':'Sentence'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NA                     122149\n",
       "genomic_alterations     32831\n",
       "biomarker               20145\n",
       "therapeutic              3139\n",
       "Name: Relation, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Relation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NA                     15608\n",
       "biomarker               2315\n",
       "genomic_alterations     2209\n",
       "therapeutic              384\n",
       "Name: Relation, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Relation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Relation = df.Relation.replace({'NA':0,\n",
    "                         'genomic_alterations':1,\n",
    "                         'biomarker':2,\n",
    "                         'therapeutic':3,\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.Relation = test_df.Relation.replace({'NA':0,\n",
    "                         'genomic_alterations':1,\n",
    "                         'biomarker':2,\n",
    "                         'therapeutic':3,\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df.groupby('Relation').apply(lambda x: x.sample(40000,replace=True,random_state=1)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "  data=[]\n",
    "  for i in range(len(df['Relation'])):\n",
    "    data.append(nltk.word_tokenize(df['Sentence'][i]))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenize(df)\n",
    "test_data = tokenize(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " '@',\n",
       " '*',\n",
       " 'gene',\n",
       " '*',\n",
       " 'monocyte',\n",
       " 'chemoattractant',\n",
       " 'protein-1',\n",
       " '@',\n",
       " 'gene',\n",
       " 'polymorphism',\n",
       " 'is',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'occult',\n",
       " 'ischemia',\n",
       " 'in',\n",
       " '#',\n",
       " '^',\n",
       " 'disease',\n",
       " '^',\n",
       " 'a',\n",
       " 'high-risk',\n",
       " '#',\n",
       " 'asymptomatic',\n",
       " 'population',\n",
       " '.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vectors(data):\n",
    "    X = []\n",
    "    for review in data:\n",
    "        word_vectors = []\n",
    "        for word in review:\n",
    "            try:\n",
    "                word_vectors.append(model[word])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if not word_vectors:\n",
    "            X.append(np.zeros(300))\n",
    "        else:\n",
    "            X.append(np.mean(word_vectors, axis=0))\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traind, y_train= data,df['Relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " '@',\n",
       " '*',\n",
       " 'gene',\n",
       " '*',\n",
       " 'monocyte',\n",
       " 'chemoattractant',\n",
       " 'protein-1',\n",
       " '@',\n",
       " 'gene',\n",
       " 'polymorphism',\n",
       " 'is',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'occult',\n",
       " 'ischemia',\n",
       " 'in',\n",
       " '#',\n",
       " '^',\n",
       " 'disease',\n",
       " '^',\n",
       " 'a',\n",
       " 'high-risk',\n",
       " '#',\n",
       " 'asymptomatic',\n",
       " 'population',\n",
       " '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_traind[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testd, y_test = test_data , test_df['Relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1,y_train1 = df['Sentence'],df['Relation']\n",
    "X_test1,y_test1 = test_df['Sentence'], test_df['Relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178264 20516\n",
      "(178264,) (20516,)\n"
     ]
    }
   ],
   "source": [
    "print(len(X_traind),len(X_testd))\n",
    "print(X_train1.shape,X_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=TfidfVectorizer(ngram_range=(1,2),\n",
    "                        use_idf=True).fit(df['Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized=vect.transform(X_train1)\n",
    "X_test_vectorized = vect.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178264, 1213019)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.astype('int')\n",
    "y_test=y_test.astype('int')\n",
    "y_train1=y_train1.astype('int')\n",
    "y_test1=y_test1.astype('int')\n",
    "X_train=generate_vectors(X_traind)\n",
    "X_test=generate_vectors(X_testd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron - With TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_tfidf_clf = Perceptron()\n",
    "perceptron_tfidf_clf.fit(X_train_vectorized, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90     15608\n",
      "           1       0.58      0.70      0.63      2209\n",
      "           2       0.45      0.19      0.27      2315\n",
      "           3       0.43      0.24      0.30       384\n",
      "\n",
      "    accuracy                           0.81     20516\n",
      "   macro avg       0.58      0.51      0.53     20516\n",
      "weighted avg       0.79      0.81      0.79     20516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perceptron_tfidf_pred=perceptron_tfidf_clf.predict(vect.transform(X_test1))\n",
    "print(classification_report(y_test1,perceptron_tfidf_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_clf = Perceptron()\n",
    "perceptron_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87     15608\n",
      "           1       0.50      0.44      0.47      2209\n",
      "           2       0.34      0.09      0.14      2315\n",
      "           3       0.50      0.03      0.05       384\n",
      "\n",
      "    accuracy                           0.77     20516\n",
      "   macro avg       0.54      0.37      0.38     20516\n",
      "weighted avg       0.72      0.77      0.73     20516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perceptron_pred=perceptron_clf.predict(X_test)\n",
    "print(classification_report(y_test, perceptron_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM With TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_tfidf_clf=LinearSVC()\n",
    "svm_tfidf_clf.fit(X_train_vectorized, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8648    0.9514    0.9060     15608\n",
      "           1     0.6228    0.6913    0.6552      2209\n",
      "           2     0.5081    0.1624    0.2462      2315\n",
      "           3     0.5033    0.2005    0.2868       384\n",
      "\n",
      "    accuracy                         0.8203     20516\n",
      "   macro avg     0.6247    0.5014    0.5235     20516\n",
      "weighted avg     0.7917    0.8203    0.7930     20516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_tfidf_pred=svm_tfidf_clf.predict(vect.transform(X_test1))\n",
    "print(classification_report(y_test1,svm_tfidf_pred,digits=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf=LinearSVC()\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8116    0.9461    0.8737     15608\n",
      "           1     0.5185    0.4685    0.4923      2209\n",
      "           2     0.3275    0.0406    0.0723      2315\n",
      "           3     0.3077    0.0312    0.0567       384\n",
      "\n",
      "    accuracy                         0.7754     20516\n",
      "   macro avg     0.4913    0.3716    0.3737     20516\n",
      "weighted avg     0.7160    0.7754    0.7269     20516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_pred=svm_clf.predict(X_test)\n",
    "print(classification_report(y_test, svm_pred,digits=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.softmax(self.fc3(out))\n",
    "        return out\n",
    "def call_MLP(inputdim,X_train,X_test,y_train,y_test):\n",
    "    input_dim = inputdim\n",
    "    hidden_dim1 = 256\n",
    "    hidden_dim2 = 64\n",
    "    output_dim = 4 \n",
    "    learning_rate = 0.05\n",
    "    num_epochs = 10\n",
    "    batch_size = 64\n",
    "\n",
    "    train_data = X_train\n",
    "    train_labels = y_train.to_numpy()\n",
    "    test_data = X_test\n",
    "    test_labels = y_test.to_numpy()\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_data).float().to(device), torch.from_numpy(train_labels).long().to(device))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    MLP_model = MLP(input_dim, hidden_dim1, hidden_dim2, output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.Tensor([1,2,4,8]).to(device))\n",
    "    optimizer = optim.Adam(MLP_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = MLP_model(inputs.to(device))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        MLP_model.eval()\n",
    "        test_data = torch.from_numpy(test_data).float().to(device)\n",
    "        test_labels = torch.from_numpy(test_labels).long().to(device)\n",
    "        outputs = MLP_model(test_data).to(device)\n",
    "        outputs_cpu = outputs.cpu()\n",
    "        _, predicted = torch.max(outputs_cpu.data, 1)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "\n",
    "        total = test_labels.size(0)\n",
    "        correct = (predicted == test_labels.cpu().numpy()).sum().item()\n",
    "        print(classification_report(predicted,test_labels.cpu().numpy(),digits=4))\n",
    "        print('Test Accuracy: {:.2f}%'.format(100 * correct / total))\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [100/2786], Loss: 1.3872\n",
      "Epoch [1/10], Batch [200/2786], Loss: 1.4279\n",
      "Epoch [1/10], Batch [300/2786], Loss: 1.3958\n",
      "Epoch [1/10], Batch [400/2786], Loss: 1.5716\n",
      "Epoch [1/10], Batch [500/2786], Loss: 1.4134\n",
      "Epoch [1/10], Batch [600/2786], Loss: 1.4510\n",
      "Epoch [1/10], Batch [700/2786], Loss: 1.3676\n",
      "Epoch [1/10], Batch [800/2786], Loss: 1.4654\n",
      "Epoch [1/10], Batch [900/2786], Loss: 1.3676\n",
      "Epoch [1/10], Batch [1000/2786], Loss: 1.4103\n",
      "Epoch [1/10], Batch [1100/2786], Loss: 1.5060\n",
      "Epoch [1/10], Batch [1200/2786], Loss: 1.4474\n",
      "Epoch [1/10], Batch [1300/2786], Loss: 1.4520\n",
      "Epoch [1/10], Batch [1400/2786], Loss: 1.3577\n",
      "Epoch [1/10], Batch [1500/2786], Loss: 1.4605\n",
      "Epoch [1/10], Batch [1600/2786], Loss: 1.3363\n",
      "Epoch [1/10], Batch [1700/2786], Loss: 1.5275\n",
      "Epoch [1/10], Batch [1800/2786], Loss: 1.3865\n",
      "Epoch [1/10], Batch [1900/2786], Loss: 1.4835\n",
      "Epoch [1/10], Batch [2000/2786], Loss: 1.3591\n",
      "Epoch [1/10], Batch [2100/2786], Loss: 1.4103\n",
      "Epoch [1/10], Batch [2200/2786], Loss: 1.3800\n",
      "Epoch [1/10], Batch [2300/2786], Loss: 1.6517\n",
      "Epoch [1/10], Batch [2400/2786], Loss: 1.6237\n",
      "Epoch [1/10], Batch [2500/2786], Loss: 1.4237\n",
      "Epoch [1/10], Batch [2600/2786], Loss: 1.5037\n",
      "Epoch [1/10], Batch [2700/2786], Loss: 1.3286\n",
      "Epoch [2/10], Batch [100/2786], Loss: 1.5214\n",
      "Epoch [2/10], Batch [200/2786], Loss: 1.3437\n",
      "Epoch [2/10], Batch [300/2786], Loss: 1.3942\n",
      "Epoch [2/10], Batch [400/2786], Loss: 1.5532\n",
      "Epoch [2/10], Batch [500/2786], Loss: 1.3663\n",
      "Epoch [2/10], Batch [600/2786], Loss: 1.4664\n",
      "Epoch [2/10], Batch [700/2786], Loss: 1.4550\n",
      "Epoch [2/10], Batch [800/2786], Loss: 1.3999\n",
      "Epoch [2/10], Batch [900/2786], Loss: 1.4489\n",
      "Epoch [2/10], Batch [1000/2786], Loss: 1.4580\n",
      "Epoch [2/10], Batch [1100/2786], Loss: 1.4550\n",
      "Epoch [2/10], Batch [1200/2786], Loss: 1.5173\n",
      "Epoch [2/10], Batch [1300/2786], Loss: 1.6260\n",
      "Epoch [2/10], Batch [1400/2786], Loss: 1.3770\n",
      "Epoch [2/10], Batch [1500/2786], Loss: 1.4330\n",
      "Epoch [2/10], Batch [1600/2786], Loss: 1.4393\n",
      "Epoch [2/10], Batch [1700/2786], Loss: 1.4718\n",
      "Epoch [2/10], Batch [1800/2786], Loss: 1.4828\n",
      "Epoch [2/10], Batch [1900/2786], Loss: 1.3999\n",
      "Epoch [2/10], Batch [2000/2786], Loss: 1.5037\n",
      "Epoch [2/10], Batch [2100/2786], Loss: 1.6603\n",
      "Epoch [2/10], Batch [2200/2786], Loss: 1.3508\n",
      "Epoch [2/10], Batch [2300/2786], Loss: 1.4418\n",
      "Epoch [2/10], Batch [2400/2786], Loss: 1.5350\n",
      "Epoch [2/10], Batch [2500/2786], Loss: 1.4605\n",
      "Epoch [2/10], Batch [2600/2786], Loss: 1.5331\n",
      "Epoch [2/10], Batch [2700/2786], Loss: 1.4333\n",
      "Epoch [3/10], Batch [100/2786], Loss: 1.5084\n",
      "Epoch [3/10], Batch [200/2786], Loss: 1.5263\n",
      "Epoch [3/10], Batch [300/2786], Loss: 1.4047\n",
      "Epoch [3/10], Batch [400/2786], Loss: 1.4692\n",
      "Epoch [3/10], Batch [500/2786], Loss: 1.5883\n",
      "Epoch [3/10], Batch [600/2786], Loss: 1.5060\n",
      "Epoch [3/10], Batch [700/2786], Loss: 1.4725\n",
      "Epoch [3/10], Batch [800/2786], Loss: 1.5770\n",
      "Epoch [3/10], Batch [900/2786], Loss: 1.4411\n",
      "Epoch [3/10], Batch [1000/2786], Loss: 1.4637\n",
      "Epoch [3/10], Batch [1100/2786], Loss: 1.4193\n",
      "Epoch [3/10], Batch [1200/2786], Loss: 1.6567\n",
      "Epoch [3/10], Batch [1300/2786], Loss: 1.6103\n",
      "Epoch [3/10], Batch [1400/2786], Loss: 1.4520\n",
      "Epoch [3/10], Batch [1500/2786], Loss: 1.5698\n",
      "Epoch [3/10], Batch [1600/2786], Loss: 1.5179\n",
      "Epoch [3/10], Batch [1700/2786], Loss: 1.3872\n",
      "Epoch [3/10], Batch [1800/2786], Loss: 1.5255\n",
      "Epoch [3/10], Batch [1900/2786], Loss: 1.4856\n",
      "Epoch [3/10], Batch [2000/2786], Loss: 1.2650\n",
      "Epoch [3/10], Batch [2100/2786], Loss: 1.4299\n",
      "Epoch [3/10], Batch [2200/2786], Loss: 1.4446\n",
      "Epoch [3/10], Batch [2300/2786], Loss: 1.3800\n",
      "Epoch [3/10], Batch [2400/2786], Loss: 1.3553\n",
      "Epoch [3/10], Batch [2500/2786], Loss: 1.5275\n",
      "Epoch [3/10], Batch [2600/2786], Loss: 1.5598\n",
      "Epoch [3/10], Batch [2700/2786], Loss: 1.4692\n",
      "Epoch [4/10], Batch [100/2786], Loss: 1.4678\n",
      "Epoch [4/10], Batch [200/2786], Loss: 1.4580\n",
      "Epoch [4/10], Batch [300/2786], Loss: 1.4981\n",
      "Epoch [4/10], Batch [400/2786], Loss: 1.4937\n",
      "Epoch [4/10], Batch [500/2786], Loss: 1.4678\n",
      "Epoch [4/10], Batch [600/2786], Loss: 1.5550\n",
      "Epoch [4/10], Batch [700/2786], Loss: 1.4474\n",
      "Epoch [4/10], Batch [800/2786], Loss: 1.4718\n",
      "Epoch [4/10], Batch [900/2786], Loss: 1.5532\n",
      "Epoch [4/10], Batch [1000/2786], Loss: 1.3243\n",
      "Epoch [4/10], Batch [1100/2786], Loss: 1.4770\n",
      "Epoch [4/10], Batch [1200/2786], Loss: 1.6473\n",
      "Epoch [4/10], Batch [1300/2786], Loss: 1.4360\n",
      "Epoch [4/10], Batch [1400/2786], Loss: 1.4138\n",
      "Epoch [4/10], Batch [1500/2786], Loss: 1.3865\n",
      "Epoch [4/10], Batch [1600/2786], Loss: 1.6174\n",
      "Epoch [4/10], Batch [1700/2786], Loss: 1.3698\n",
      "Epoch [4/10], Batch [1800/2786], Loss: 1.4770\n",
      "Epoch [4/10], Batch [1900/2786], Loss: 1.4988\n",
      "Epoch [4/10], Batch [2000/2786], Loss: 1.4330\n",
      "Epoch [4/10], Batch [2100/2786], Loss: 1.5084\n",
      "Epoch [4/10], Batch [2200/2786], Loss: 1.5214\n",
      "Epoch [4/10], Batch [2300/2786], Loss: 1.5129\n",
      "Epoch [4/10], Batch [2400/2786], Loss: 1.4678\n",
      "Epoch [4/10], Batch [2500/2786], Loss: 1.3942\n",
      "Epoch [4/10], Batch [2600/2786], Loss: 1.4937\n",
      "Epoch [4/10], Batch [2700/2786], Loss: 1.3363\n",
      "Epoch [5/10], Batch [100/2786], Loss: 1.4279\n",
      "Epoch [5/10], Batch [200/2786], Loss: 1.3958\n",
      "Epoch [5/10], Batch [300/2786], Loss: 1.4510\n",
      "Epoch [5/10], Batch [400/2786], Loss: 1.4501\n",
      "Epoch [5/10], Batch [500/2786], Loss: 1.2954\n",
      "Epoch [5/10], Batch [600/2786], Loss: 1.4040\n",
      "Epoch [5/10], Batch [700/2786], Loss: 1.5532\n",
      "Epoch [5/10], Batch [800/2786], Loss: 1.5266\n",
      "Epoch [5/10], Batch [900/2786], Loss: 1.5678\n",
      "Epoch [5/10], Batch [1000/2786], Loss: 1.4032\n",
      "Epoch [5/10], Batch [1100/2786], Loss: 1.4962\n",
      "Epoch [5/10], Batch [1200/2786], Loss: 1.4718\n",
      "Epoch [5/10], Batch [1300/2786], Loss: 1.4393\n",
      "Epoch [5/10], Batch [1400/2786], Loss: 1.6125\n",
      "Epoch [5/10], Batch [1500/2786], Loss: 1.6132\n",
      "Epoch [5/10], Batch [1600/2786], Loss: 1.4520\n",
      "Epoch [5/10], Batch [1700/2786], Loss: 1.5012\n",
      "Epoch [5/10], Batch [1800/2786], Loss: 1.5375\n",
      "Epoch [5/10], Batch [1900/2786], Loss: 1.4068\n",
      "Epoch [5/10], Batch [2000/2786], Loss: 1.5044\n",
      "Epoch [5/10], Batch [2100/2786], Loss: 1.4664\n",
      "Epoch [5/10], Batch [2200/2786], Loss: 1.4386\n",
      "Epoch [5/10], Batch [2300/2786], Loss: 1.3189\n",
      "Epoch [5/10], Batch [2400/2786], Loss: 1.4501\n",
      "Epoch [5/10], Batch [2500/2786], Loss: 1.4360\n",
      "Epoch [5/10], Batch [2600/2786], Loss: 1.5368\n",
      "Epoch [5/10], Batch [2700/2786], Loss: 1.6187\n",
      "Epoch [6/10], Batch [100/2786], Loss: 1.4474\n",
      "Epoch [6/10], Batch [200/2786], Loss: 1.5275\n",
      "Epoch [6/10], Batch [300/2786], Loss: 1.5313\n",
      "Epoch [6/10], Batch [400/2786], Loss: 1.5618\n",
      "Epoch [6/10], Batch [500/2786], Loss: 1.4844\n",
      "Epoch [6/10], Batch [600/2786], Loss: 1.4608\n",
      "Epoch [6/10], Batch [700/2786], Loss: 1.3473\n",
      "Epoch [6/10], Batch [800/2786], Loss: 1.5331\n",
      "Epoch [6/10], Batch [900/2786], Loss: 1.4770\n",
      "Epoch [6/10], Batch [1000/2786], Loss: 1.4988\n",
      "Epoch [6/10], Batch [1100/2786], Loss: 1.3663\n",
      "Epoch [6/10], Batch [1200/2786], Loss: 1.4718\n",
      "Epoch [6/10], Batch [1300/2786], Loss: 1.5598\n",
      "Epoch [6/10], Batch [1400/2786], Loss: 1.5698\n",
      "Epoch [6/10], Batch [1500/2786], Loss: 1.5550\n",
      "Epoch [6/10], Batch [1600/2786], Loss: 1.3800\n",
      "Epoch [6/10], Batch [1700/2786], Loss: 1.4975\n",
      "Epoch [6/10], Batch [1800/2786], Loss: 1.5716\n",
      "Epoch [6/10], Batch [1900/2786], Loss: 1.3403\n",
      "Epoch [6/10], Batch [2000/2786], Loss: 1.5331\n",
      "Epoch [6/10], Batch [2100/2786], Loss: 1.4251\n",
      "Epoch [6/10], Batch [2200/2786], Loss: 1.4237\n",
      "Epoch [6/10], Batch [2300/2786], Loss: 1.5129\n",
      "Epoch [6/10], Batch [2400/2786], Loss: 1.4605\n",
      "Epoch [6/10], Batch [2500/2786], Loss: 1.4262\n",
      "Epoch [6/10], Batch [2600/2786], Loss: 1.5416\n",
      "Epoch [6/10], Batch [2700/2786], Loss: 1.5770\n",
      "Epoch [7/10], Batch [100/2786], Loss: 1.3189\n",
      "Epoch [7/10], Batch [200/2786], Loss: 1.3363\n",
      "Epoch [7/10], Batch [300/2786], Loss: 1.5164\n",
      "Epoch [7/10], Batch [400/2786], Loss: 1.3644\n",
      "Epoch [7/10], Batch [500/2786], Loss: 1.4844\n",
      "Epoch [7/10], Batch [600/2786], Loss: 1.3928\n",
      "Epoch [7/10], Batch [700/2786], Loss: 1.3800\n",
      "Epoch [7/10], Batch [800/2786], Loss: 1.5084\n",
      "Epoch [7/10], Batch [900/2786], Loss: 1.4981\n",
      "Epoch [7/10], Batch [1000/2786], Loss: 1.4171\n",
      "Epoch [7/10], Batch [1100/2786], Loss: 1.4937\n",
      "Epoch [7/10], Batch [1200/2786], Loss: 1.5476\n",
      "Epoch [7/10], Batch [1300/2786], Loss: 1.5698\n",
      "Epoch [7/10], Batch [1400/2786], Loss: 1.5639\n",
      "Epoch [7/10], Batch [1500/2786], Loss: 1.4040\n",
      "Epoch [7/10], Batch [1600/2786], Loss: 1.6283\n",
      "Epoch [7/10], Batch [1700/2786], Loss: 1.4988\n",
      "Epoch [7/10], Batch [1800/2786], Loss: 1.3400\n",
      "Epoch [7/10], Batch [1900/2786], Loss: 1.5514\n",
      "Epoch [7/10], Batch [2000/2786], Loss: 1.3837\n",
      "Epoch [7/10], Batch [2100/2786], Loss: 1.4171\n",
      "Epoch [7/10], Batch [2200/2786], Loss: 1.5331\n",
      "Epoch [7/10], Batch [2300/2786], Loss: 1.5585\n",
      "Epoch [7/10], Batch [2400/2786], Loss: 1.4520\n",
      "Epoch [7/10], Batch [2500/2786], Loss: 1.4333\n",
      "Epoch [7/10], Batch [2600/2786], Loss: 1.3515\n",
      "Epoch [7/10], Batch [2700/2786], Loss: 1.5037\n",
      "Epoch [8/10], Batch [100/2786], Loss: 1.3733\n",
      "Epoch [8/10], Batch [200/2786], Loss: 1.5255\n",
      "Epoch [8/10], Batch [300/2786], Loss: 1.6237\n",
      "Epoch [8/10], Batch [400/2786], Loss: 1.4795\n",
      "Epoch [8/10], Batch [500/2786], Loss: 1.4251\n",
      "Epoch [8/10], Batch [600/2786], Loss: 1.5012\n",
      "Epoch [8/10], Batch [700/2786], Loss: 1.3733\n",
      "Epoch [8/10], Batch [800/2786], Loss: 1.5129\n",
      "Epoch [8/10], Batch [900/2786], Loss: 1.3437\n",
      "Epoch [8/10], Batch [1000/2786], Loss: 1.4554\n",
      "Epoch [8/10], Batch [1100/2786], Loss: 1.4389\n",
      "Epoch [8/10], Batch [1200/2786], Loss: 1.3958\n",
      "Epoch [8/10], Batch [1300/2786], Loss: 1.6393\n",
      "Epoch [8/10], Batch [1400/2786], Loss: 1.3553\n",
      "Epoch [8/10], Batch [1500/2786], Loss: 1.5618\n",
      "Epoch [8/10], Batch [1600/2786], Loss: 1.3577\n",
      "Epoch [8/10], Batch [1700/2786], Loss: 1.5495\n",
      "Epoch [8/10], Batch [1800/2786], Loss: 1.3837\n",
      "Epoch [8/10], Batch [1900/2786], Loss: 1.4702\n",
      "Epoch [8/10], Batch [2000/2786], Loss: 1.4692\n",
      "Epoch [8/10], Batch [2100/2786], Loss: 1.4580\n",
      "Epoch [8/10], Batch [2200/2786], Loss: 1.5275\n",
      "Epoch [8/10], Batch [2300/2786], Loss: 1.5023\n",
      "Epoch [8/10], Batch [2400/2786], Loss: 1.5837\n",
      "Epoch [8/10], Batch [2500/2786], Loss: 1.3833\n",
      "Epoch [8/10], Batch [2600/2786], Loss: 1.5286\n",
      "Epoch [8/10], Batch [2700/2786], Loss: 1.5214\n",
      "Epoch [9/10], Batch [100/2786], Loss: 1.3942\n",
      "Epoch [9/10], Batch [200/2786], Loss: 1.5514\n",
      "Epoch [9/10], Batch [300/2786], Loss: 1.4185\n",
      "Epoch [9/10], Batch [400/2786], Loss: 1.6146\n",
      "Epoch [9/10], Batch [500/2786], Loss: 1.4446\n",
      "Epoch [9/10], Batch [600/2786], Loss: 1.4637\n",
      "Epoch [9/10], Batch [700/2786], Loss: 1.4630\n",
      "Epoch [9/10], Batch [800/2786], Loss: 1.3073\n",
      "Epoch [9/10], Batch [900/2786], Loss: 1.5941\n",
      "Epoch [9/10], Batch [1000/2786], Loss: 1.5476\n",
      "Epoch [9/10], Batch [1100/2786], Loss: 1.5501\n",
      "Epoch [9/10], Batch [1200/2786], Loss: 1.5456\n",
      "Epoch [9/10], Batch [1300/2786], Loss: 1.3286\n",
      "Epoch [9/10], Batch [1400/2786], Loss: 1.4333\n",
      "Epoch [9/10], Batch [1500/2786], Loss: 1.4868\n",
      "Epoch [9/10], Batch [1600/2786], Loss: 1.3800\n",
      "Epoch [9/10], Batch [1700/2786], Loss: 1.4134\n",
      "Epoch [9/10], Batch [1800/2786], Loss: 1.4748\n",
      "Epoch [9/10], Batch [1900/2786], Loss: 1.4868\n",
      "Epoch [9/10], Batch [2000/2786], Loss: 1.5331\n",
      "Epoch [9/10], Batch [2100/2786], Loss: 1.5735\n",
      "Epoch [9/10], Batch [2200/2786], Loss: 1.4910\n",
      "Epoch [9/10], Batch [2300/2786], Loss: 1.4164\n",
      "Epoch [9/10], Batch [2400/2786], Loss: 1.5037\n",
      "Epoch [9/10], Batch [2500/2786], Loss: 1.4637\n",
      "Epoch [9/10], Batch [2600/2786], Loss: 1.4520\n",
      "Epoch [9/10], Batch [2700/2786], Loss: 1.4204\n",
      "Epoch [10/10], Batch [100/2786], Loss: 1.3897\n",
      "Epoch [10/10], Batch [200/2786], Loss: 1.3975\n",
      "Epoch [10/10], Batch [300/2786], Loss: 1.4550\n",
      "Epoch [10/10], Batch [400/2786], Loss: 1.5331\n",
      "Epoch [10/10], Batch [500/2786], Loss: 1.4446\n",
      "Epoch [10/10], Batch [600/2786], Loss: 1.4008\n",
      "Epoch [10/10], Batch [700/2786], Loss: 1.5456\n",
      "Epoch [10/10], Batch [800/2786], Loss: 1.4897\n",
      "Epoch [10/10], Batch [900/2786], Loss: 1.5331\n",
      "Epoch [10/10], Batch [1000/2786], Loss: 1.4770\n",
      "Epoch [10/10], Batch [1100/2786], Loss: 1.4418\n",
      "Epoch [10/10], Batch [1200/2786], Loss: 1.3865\n",
      "Epoch [10/10], Batch [1300/2786], Loss: 1.4279\n",
      "Epoch [10/10], Batch [1400/2786], Loss: 1.4554\n",
      "Epoch [10/10], Batch [1500/2786], Loss: 1.5787\n",
      "Epoch [10/10], Batch [1600/2786], Loss: 1.5385\n",
      "Epoch [10/10], Batch [1700/2786], Loss: 1.4988\n",
      "Epoch [10/10], Batch [1800/2786], Loss: 1.4580\n",
      "Epoch [10/10], Batch [1900/2786], Loss: 1.3708\n",
      "Epoch [10/10], Batch [2000/2786], Loss: 1.5514\n",
      "Epoch [10/10], Batch [2100/2786], Loss: 1.6174\n",
      "Epoch [10/10], Batch [2200/2786], Loss: 1.5770\n",
      "Epoch [10/10], Batch [2300/2786], Loss: 1.6620\n",
      "Epoch [10/10], Batch [2400/2786], Loss: 1.2588\n",
      "Epoch [10/10], Batch [2500/2786], Loss: 1.5770\n",
      "Epoch [10/10], Batch [2600/2786], Loss: 1.4158\n",
      "Epoch [10/10], Batch [2700/2786], Loss: 1.4520\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     0.0000    0.0000    0.0000         0\n",
      "           2     1.0000    0.1128    0.2028     20516\n",
      "           3     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.1128     20516\n",
      "   macro avg     0.2500    0.0282    0.0507     20516\n",
      "weighted avg     1.0000    0.1128    0.2028     20516\n",
      "\n",
      "Test Accuracy: 11.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "outputs=call_MLP(300,X_train,X_test,y_train,y_test)\n",
    "# print(classification_report(predicted,test_labels.cpu().numpy()))\n",
    "# print('Test Accuracy: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vectors_3(data):\n",
    "    inputs = []\n",
    "    for review in data:\n",
    "        review_vecs = []\n",
    "        for i, word in enumerate(review):\n",
    "            # print(word in model)\n",
    "            if i < 20:\n",
    "                if word in model:\n",
    "                    review_vecs.append(model[word])\n",
    "        if len(review_vecs) < 20:\n",
    "            review_vecs += [np.zeros(300)] * (20 - len(review_vecs))\n",
    "        inputs.append(review_vecs)\n",
    "    return np.array(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train3\u001b[39m=\u001b[39mgenerate_vectors_3(X_traind)\n\u001b[0;32m      2\u001b[0m X_test3\u001b[39m=\u001b[39mgenerate_vectors_3(X_testd)\n",
      "Cell \u001b[1;32mIn[92], line 13\u001b[0m, in \u001b[0;36mgenerate_vectors_3\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     11\u001b[0m         review_vecs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mzeros(\u001b[39m300\u001b[39m)] \u001b[39m*\u001b[39m (\u001b[39m20\u001b[39m \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(review_vecs))\n\u001b[0;32m     12\u001b[0m     inputs\u001b[39m.\u001b[39mappend(review_vecs)\n\u001b[1;32m---> 13\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49marray(inputs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train3=generate_vectors_3(X_traind)\n",
    "X_test3=generate_vectors_3(X_testd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "num_samples = [122149, 32831, 20145, 3139] # number of samples in each class\n",
    "total_samples = sum(num_samples)\n",
    "class_weights = torch.Tensor([total_samples / num_samples[i] for i in range(len(num_samples))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4594,  5.4297,  8.8490, 56.7901])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Batch [100/2786], Loss: 0.8649\n",
      "Epoch [1/15], Batch [200/2786], Loss: 1.0431\n",
      "Epoch [1/15], Batch [300/2786], Loss: 1.0390\n",
      "Epoch [1/15], Batch [400/2786], Loss: 0.7495\n",
      "Epoch [1/15], Batch [500/2786], Loss: 0.8866\n",
      "Epoch [1/15], Batch [600/2786], Loss: 0.8658\n",
      "Epoch [1/15], Batch [700/2786], Loss: 0.8176\n",
      "Epoch [1/15], Batch [800/2786], Loss: 0.8524\n",
      "Epoch [1/15], Batch [900/2786], Loss: 0.7771\n",
      "Epoch [1/15], Batch [1000/2786], Loss: 0.7158\n",
      "Epoch [1/15], Batch [1100/2786], Loss: 0.7328\n",
      "Epoch [1/15], Batch [1200/2786], Loss: 0.9519\n",
      "Epoch [1/15], Batch [1300/2786], Loss: 0.6836\n",
      "Epoch [1/15], Batch [1400/2786], Loss: 0.6499\n",
      "Epoch [1/15], Batch [1500/2786], Loss: 0.6568\n",
      "Epoch [1/15], Batch [1600/2786], Loss: 0.8016\n",
      "Epoch [1/15], Batch [1700/2786], Loss: 0.6694\n",
      "Epoch [1/15], Batch [1800/2786], Loss: 0.9220\n",
      "Epoch [1/15], Batch [1900/2786], Loss: 0.8149\n",
      "Epoch [1/15], Batch [2000/2786], Loss: 0.6134\n",
      "Epoch [1/15], Batch [2100/2786], Loss: 0.8591\n",
      "Epoch [1/15], Batch [2200/2786], Loss: 0.8446\n",
      "Epoch [1/15], Batch [2300/2786], Loss: 0.8069\n",
      "Epoch [1/15], Batch [2400/2786], Loss: 0.7557\n",
      "Epoch [1/15], Batch [2500/2786], Loss: 0.5421\n",
      "Epoch [1/15], Batch [2600/2786], Loss: 0.7045\n",
      "Epoch [1/15], Batch [2700/2786], Loss: 0.7455\n",
      "Epoch [2/15], Batch [100/2786], Loss: 0.7556\n",
      "Epoch [2/15], Batch [200/2786], Loss: 0.7575\n",
      "Epoch [2/15], Batch [300/2786], Loss: 0.8451\n",
      "Epoch [2/15], Batch [400/2786], Loss: 0.6489\n",
      "Epoch [2/15], Batch [500/2786], Loss: 0.7175\n",
      "Epoch [2/15], Batch [600/2786], Loss: 0.7335\n",
      "Epoch [2/15], Batch [700/2786], Loss: 0.8101\n",
      "Epoch [2/15], Batch [800/2786], Loss: 0.8498\n",
      "Epoch [2/15], Batch [900/2786], Loss: 0.6440\n",
      "Epoch [2/15], Batch [1000/2786], Loss: 0.8698\n",
      "Epoch [2/15], Batch [1100/2786], Loss: 0.7909\n",
      "Epoch [2/15], Batch [1200/2786], Loss: 0.9058\n",
      "Epoch [2/15], Batch [1300/2786], Loss: 0.7381\n",
      "Epoch [2/15], Batch [1400/2786], Loss: 0.7916\n",
      "Epoch [2/15], Batch [1500/2786], Loss: 0.7555\n",
      "Epoch [2/15], Batch [1600/2786], Loss: 0.6912\n",
      "Epoch [2/15], Batch [1700/2786], Loss: 0.5889\n",
      "Epoch [2/15], Batch [1800/2786], Loss: 0.7226\n",
      "Epoch [2/15], Batch [1900/2786], Loss: 0.8439\n",
      "Epoch [2/15], Batch [2000/2786], Loss: 0.6484\n",
      "Epoch [2/15], Batch [2100/2786], Loss: 0.8434\n",
      "Epoch [2/15], Batch [2200/2786], Loss: 0.8293\n",
      "Epoch [2/15], Batch [2300/2786], Loss: 0.6383\n",
      "Epoch [2/15], Batch [2400/2786], Loss: 0.8475\n",
      "Epoch [2/15], Batch [2500/2786], Loss: 0.6555\n",
      "Epoch [2/15], Batch [2600/2786], Loss: 0.7940\n",
      "Epoch [2/15], Batch [2700/2786], Loss: 0.5202\n",
      "Epoch [3/15], Batch [100/2786], Loss: 0.6566\n",
      "Epoch [3/15], Batch [200/2786], Loss: 0.7228\n",
      "Epoch [3/15], Batch [300/2786], Loss: 0.7366\n",
      "Epoch [3/15], Batch [400/2786], Loss: 0.7637\n",
      "Epoch [3/15], Batch [500/2786], Loss: 0.6461\n",
      "Epoch [3/15], Batch [600/2786], Loss: 0.7037\n",
      "Epoch [3/15], Batch [700/2786], Loss: 0.7453\n",
      "Epoch [3/15], Batch [800/2786], Loss: 0.6128\n",
      "Epoch [3/15], Batch [900/2786], Loss: 0.7077\n",
      "Epoch [3/15], Batch [1000/2786], Loss: 0.7646\n",
      "Epoch [3/15], Batch [1100/2786], Loss: 0.8412\n",
      "Epoch [3/15], Batch [1200/2786], Loss: 0.7682\n",
      "Epoch [3/15], Batch [1300/2786], Loss: 0.6404\n",
      "Epoch [3/15], Batch [1400/2786], Loss: 0.7059\n",
      "Epoch [3/15], Batch [1500/2786], Loss: 0.6691\n",
      "Epoch [3/15], Batch [1600/2786], Loss: 0.8060\n",
      "Epoch [3/15], Batch [1700/2786], Loss: 0.6419\n",
      "Epoch [3/15], Batch [1800/2786], Loss: 0.8055\n",
      "Epoch [3/15], Batch [1900/2786], Loss: 0.5772\n",
      "Epoch [3/15], Batch [2000/2786], Loss: 0.6603\n",
      "Epoch [3/15], Batch [2100/2786], Loss: 0.6488\n",
      "Epoch [3/15], Batch [2200/2786], Loss: 0.7828\n",
      "Epoch [3/15], Batch [2300/2786], Loss: 0.6621\n",
      "Epoch [3/15], Batch [2400/2786], Loss: 0.6443\n",
      "Epoch [3/15], Batch [2500/2786], Loss: 0.5626\n",
      "Epoch [3/15], Batch [2600/2786], Loss: 0.5930\n",
      "Epoch [3/15], Batch [2700/2786], Loss: 0.7105\n",
      "Epoch [4/15], Batch [100/2786], Loss: 0.8112\n",
      "Epoch [4/15], Batch [200/2786], Loss: 0.5572\n",
      "Epoch [4/15], Batch [300/2786], Loss: 0.7459\n",
      "Epoch [4/15], Batch [400/2786], Loss: 0.7289\n",
      "Epoch [4/15], Batch [500/2786], Loss: 0.7367\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     37\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 38\u001b[0m     outputs \u001b[39m=\u001b[39m RNN_model(inputs)\n\u001b[0;32m     39\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     40\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[98], line 12\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m batch_size \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m     11\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_hidden(batch_size)\n\u001b[1;32m---> 12\u001b[0m out, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, hidden)\n\u001b[0;32m     13\u001b[0m out\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:])\n\u001b[0;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:509\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_TANH\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 509\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mrnn_tanh(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    510\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional,\n\u001b[0;32m    511\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    512\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    513\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mrnn_relu(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    514\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional,\n\u001b[0;32m    515\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out=self.fc(out[:,-1,:])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden\n",
    "\n",
    "train_data = X_train3\n",
    "train_labels = y_train.to_numpy()\n",
    "test_data = X_test3\n",
    "test_labels = y_test.to_numpy()\n",
    "num_epochs = 40\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_data).float(), torch.from_numpy(train_labels).long())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)   \n",
    "\n",
    "RNN_model = RNNModel(300,4,20,1)\n",
    "optimizer = torch.optim.Adam(RNN_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = RNN_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    " \n",
    " # Test the model\n",
    "with torch.no_grad():\n",
    "    RNN_model.eval()\n",
    "    test_data = torch.from_numpy(test_data).float()\n",
    "    test_labels = torch.from_numpy(test_labels).long()\n",
    "    outputs = RNN_model(test_data)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = test_labels.size(0)\n",
    "    correct = (predicted == test_labels).sum().item()\n",
    "    print(classification_report(predicted,test_labels))\n",
    "    print('Test Accuracy: {:.2f}%'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.88      0.63      8762\n",
      "           1       0.75      0.29      0.41      5765\n",
      "           2       0.34      0.17      0.23      4568\n",
      "           3       0.38      0.10      0.16      1421\n",
      "\n",
      "    accuracy                           0.50     20516\n",
      "   macro avg       0.49      0.36      0.36     20516\n",
      "weighted avg       0.52      0.50      0.45     20516\n",
      "\n",
      "Test Accuracy: 50.22%\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predicted,test_labels))\n",
    "print('Test Accuracy: {:.2f}%'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# svd = TruncatedSVD(n_components=1000)\n",
    "# X_train_svd = svd.fit_transform(X_train_vectorized)\n",
    "# X_test_svd = svd.transform(X_test_vectorized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Batch [100/2500], Loss: 1.3744\n",
      "Epoch [1/15], Batch [200/2500], Loss: 1.1485\n",
      "Epoch [1/15], Batch [300/2500], Loss: 1.0646\n",
      "Epoch [1/15], Batch [400/2500], Loss: 1.0217\n",
      "Epoch [1/15], Batch [500/2500], Loss: 1.1138\n",
      "Epoch [1/15], Batch [600/2500], Loss: 1.0710\n",
      "Epoch [1/15], Batch [700/2500], Loss: 0.8620\n",
      "Epoch [1/15], Batch [800/2500], Loss: 0.8716\n",
      "Epoch [1/15], Batch [900/2500], Loss: 0.9431\n",
      "Epoch [1/15], Batch [1000/2500], Loss: 0.8734\n",
      "Epoch [1/15], Batch [1100/2500], Loss: 0.8179\n",
      "Epoch [1/15], Batch [1200/2500], Loss: 0.9804\n",
      "Epoch [1/15], Batch [1300/2500], Loss: 0.9815\n",
      "Epoch [1/15], Batch [1400/2500], Loss: 0.8246\n",
      "Epoch [1/15], Batch [1500/2500], Loss: 0.8464\n",
      "Epoch [1/15], Batch [1600/2500], Loss: 0.7522\n",
      "Epoch [1/15], Batch [1700/2500], Loss: 0.8195\n",
      "Epoch [1/15], Batch [1800/2500], Loss: 0.8562\n",
      "Epoch [1/15], Batch [1900/2500], Loss: 0.8578\n",
      "Epoch [1/15], Batch [2000/2500], Loss: 0.9529\n",
      "Epoch [1/15], Batch [2100/2500], Loss: 0.9138\n",
      "Epoch [1/15], Batch [2200/2500], Loss: 0.8193\n",
      "Epoch [1/15], Batch [2300/2500], Loss: 0.7770\n",
      "Epoch [1/15], Batch [2400/2500], Loss: 0.8320\n",
      "Epoch [1/15], Batch [2500/2500], Loss: 0.9241\n",
      "Epoch [2/15], Batch [100/2500], Loss: 0.7901\n",
      "Epoch [2/15], Batch [200/2500], Loss: 0.7416\n",
      "Epoch [2/15], Batch [300/2500], Loss: 0.8577\n",
      "Epoch [2/15], Batch [400/2500], Loss: 0.8468\n",
      "Epoch [2/15], Batch [500/2500], Loss: 0.9571\n",
      "Epoch [2/15], Batch [600/2500], Loss: 0.6689\n",
      "Epoch [2/15], Batch [700/2500], Loss: 0.8122\n",
      "Epoch [2/15], Batch [800/2500], Loss: 0.7705\n",
      "Epoch [2/15], Batch [900/2500], Loss: 0.9217\n",
      "Epoch [2/15], Batch [1000/2500], Loss: 0.8494\n",
      "Epoch [2/15], Batch [1100/2500], Loss: 0.8954\n",
      "Epoch [2/15], Batch [1200/2500], Loss: 0.7783\n",
      "Epoch [2/15], Batch [1300/2500], Loss: 0.8299\n",
      "Epoch [2/15], Batch [1400/2500], Loss: 0.7528\n",
      "Epoch [2/15], Batch [1500/2500], Loss: 0.7347\n",
      "Epoch [2/15], Batch [1600/2500], Loss: 0.7540\n",
      "Epoch [2/15], Batch [1700/2500], Loss: 0.6138\n",
      "Epoch [2/15], Batch [1800/2500], Loss: 1.0807\n",
      "Epoch [2/15], Batch [1900/2500], Loss: 0.6502\n",
      "Epoch [2/15], Batch [2000/2500], Loss: 0.8846\n",
      "Epoch [2/15], Batch [2100/2500], Loss: 0.8216\n",
      "Epoch [2/15], Batch [2200/2500], Loss: 0.8667\n",
      "Epoch [2/15], Batch [2300/2500], Loss: 0.9516\n",
      "Epoch [2/15], Batch [2400/2500], Loss: 0.8232\n",
      "Epoch [2/15], Batch [2500/2500], Loss: 0.6855\n",
      "Epoch [3/15], Batch [100/2500], Loss: 0.7223\n",
      "Epoch [3/15], Batch [200/2500], Loss: 0.7637\n",
      "Epoch [3/15], Batch [300/2500], Loss: 0.7703\n",
      "Epoch [3/15], Batch [400/2500], Loss: 0.8040\n",
      "Epoch [3/15], Batch [500/2500], Loss: 0.6560\n",
      "Epoch [3/15], Batch [600/2500], Loss: 0.6025\n",
      "Epoch [3/15], Batch [700/2500], Loss: 0.7316\n",
      "Epoch [3/15], Batch [800/2500], Loss: 0.6358\n",
      "Epoch [3/15], Batch [900/2500], Loss: 0.6576\n",
      "Epoch [3/15], Batch [1000/2500], Loss: 0.7094\n",
      "Epoch [3/15], Batch [1100/2500], Loss: 0.5785\n",
      "Epoch [3/15], Batch [1200/2500], Loss: 0.7433\n",
      "Epoch [3/15], Batch [1300/2500], Loss: 0.5436\n",
      "Epoch [3/15], Batch [1400/2500], Loss: 0.7210\n",
      "Epoch [3/15], Batch [1500/2500], Loss: 0.7348\n",
      "Epoch [3/15], Batch [1600/2500], Loss: 0.7540\n",
      "Epoch [3/15], Batch [1700/2500], Loss: 0.6799\n",
      "Epoch [3/15], Batch [1800/2500], Loss: 0.6014\n",
      "Epoch [3/15], Batch [1900/2500], Loss: 0.6651\n",
      "Epoch [3/15], Batch [2000/2500], Loss: 0.7823\n",
      "Epoch [3/15], Batch [2100/2500], Loss: 0.7165\n",
      "Epoch [3/15], Batch [2200/2500], Loss: 0.6762\n",
      "Epoch [3/15], Batch [2300/2500], Loss: 0.7248\n",
      "Epoch [3/15], Batch [2400/2500], Loss: 0.6499\n",
      "Epoch [3/15], Batch [2500/2500], Loss: 0.5537\n",
      "Epoch [4/15], Batch [100/2500], Loss: 0.6340\n",
      "Epoch [4/15], Batch [200/2500], Loss: 0.5517\n",
      "Epoch [4/15], Batch [300/2500], Loss: 0.6039\n",
      "Epoch [4/15], Batch [400/2500], Loss: 0.7598\n",
      "Epoch [4/15], Batch [500/2500], Loss: 0.7601\n",
      "Epoch [4/15], Batch [600/2500], Loss: 0.4806\n",
      "Epoch [4/15], Batch [700/2500], Loss: 0.8354\n",
      "Epoch [4/15], Batch [800/2500], Loss: 0.6766\n",
      "Epoch [4/15], Batch [900/2500], Loss: 0.5689\n",
      "Epoch [4/15], Batch [1000/2500], Loss: 0.7365\n",
      "Epoch [4/15], Batch [1100/2500], Loss: 0.5643\n",
      "Epoch [4/15], Batch [1200/2500], Loss: 0.5997\n",
      "Epoch [4/15], Batch [1300/2500], Loss: 0.8874\n",
      "Epoch [4/15], Batch [1400/2500], Loss: 0.5650\n",
      "Epoch [4/15], Batch [1500/2500], Loss: 0.8138\n",
      "Epoch [4/15], Batch [1600/2500], Loss: 0.7339\n",
      "Epoch [4/15], Batch [1700/2500], Loss: 0.6149\n",
      "Epoch [4/15], Batch [1800/2500], Loss: 0.5580\n",
      "Epoch [4/15], Batch [1900/2500], Loss: 0.7526\n",
      "Epoch [4/15], Batch [2000/2500], Loss: 0.7006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m batch_size \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m     38\u001b[0m hidden \u001b[39m=\u001b[39m GRU_model\u001b[39m.\u001b[39minit_hidden(batch_size) \u001b[39m# fixed the model -> GRU_model\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m outputs, hidden \u001b[39m=\u001b[39m GRU_model(inputs, hidden) \u001b[39m# fixed the model -> GRU_model\u001b[39;00m\n\u001b[0;32m     40\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     41\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[51], line 10\u001b[0m, in \u001b[0;36mGRUModel.forward\u001b[1;34m(self, x, h)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, h):\n\u001b[1;32m---> 10\u001b[0m     out, h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru(x, h)\n\u001b[0;32m     11\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m out, h\n",
      "File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:998\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    997\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 998\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    999\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m   1000\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m   1002\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:, -1]))\n",
    "        return out, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_dim)\n",
    "\n",
    "\n",
    "train_data = X_train3\n",
    "train_labels = y_train.to_numpy()\n",
    "test_data = X_test3\n",
    "test_labels = y_test.to_numpy()\n",
    "num_epochs = 40\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_data).float(), torch.from_numpy(train_labels).long())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)   \n",
    "\n",
    "GRU_model = GRUModel(300,20,4)\n",
    "optimizer = torch.optim.Adam(GRU_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "for epoch in range(num_epochs):\n",
    "    h = GRU_model.init_hidden(batch_size)\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        batch_size = inputs.size(0)\n",
    "        hidden = GRU_model.init_hidden(batch_size) # fixed the model -> GRU_model\n",
    "        outputs, hidden = GRU_model(inputs, hidden) # fixed the model -> GRU_model\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    " \n",
    " # Test the model\n",
    "with torch.no_grad():\n",
    "    GRU_model.eval()\n",
    "    test_data = torch.from_numpy(test_data).float()\n",
    "    test_labels = torch.from_numpy(test_labels).long()\n",
    "    h = GRU_model.init_hidden(test_data.shape[0])\n",
    "    outputs,h = GRU_model(test_data,h)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = test_labels.size(0)\n",
    "    correct = (predicted == test_labels).sum().item()\n",
    "    print('Test Accuracy: {:.2f}%'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.90      0.68      9430\n",
      "           1       0.80      0.32      0.46      5508\n",
      "           2       0.41      0.18      0.25      5175\n",
      "           3       0.23      0.22      0.23       403\n",
      "\n",
      "    accuracy                           0.55     20516\n",
      "   macro avg       0.49      0.41      0.40     20516\n",
      "weighted avg       0.57      0.55      0.50     20516\n",
      "\n",
      "Test Accuracy: 54.94%\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predicted,test_labels))\n",
    "print('Test Accuracy: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Venkatesh Dharmaraj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Batch [100/1250], Loss: 1.2216\n",
      "Epoch [1/15], Batch [200/1250], Loss: 1.1098\n",
      "Epoch [1/15], Batch [300/1250], Loss: 1.1045\n",
      "Epoch [1/15], Batch [400/1250], Loss: 1.0352\n",
      "Epoch [1/15], Batch [500/1250], Loss: 1.1957\n",
      "Epoch [1/15], Batch [600/1250], Loss: 0.8608\n",
      "Epoch [1/15], Batch [700/1250], Loss: 0.7943\n",
      "Epoch [1/15], Batch [800/1250], Loss: 0.9694\n",
      "Epoch [1/15], Batch [900/1250], Loss: 0.8232\n",
      "Epoch [1/15], Batch [1000/1250], Loss: 0.8560\n",
      "Epoch [1/15], Batch [1100/1250], Loss: 0.9756\n",
      "Epoch [1/15], Batch [1200/1250], Loss: 0.8900\n",
      "Epoch [2/15], Batch [100/1250], Loss: 1.0175\n",
      "Epoch [2/15], Batch [200/1250], Loss: 0.8401\n",
      "Epoch [2/15], Batch [300/1250], Loss: 0.9116\n",
      "Epoch [2/15], Batch [400/1250], Loss: 0.8208\n",
      "Epoch [2/15], Batch [500/1250], Loss: 0.8517\n",
      "Epoch [2/15], Batch [600/1250], Loss: 0.7067\n",
      "Epoch [2/15], Batch [700/1250], Loss: 0.7755\n",
      "Epoch [2/15], Batch [800/1250], Loss: 0.8036\n",
      "Epoch [2/15], Batch [900/1250], Loss: 0.7509\n",
      "Epoch [2/15], Batch [1000/1250], Loss: 0.7364\n",
      "Epoch [2/15], Batch [1100/1250], Loss: 0.9313\n",
      "Epoch [2/15], Batch [1200/1250], Loss: 0.7010\n",
      "Epoch [3/15], Batch [100/1250], Loss: 0.7647\n",
      "Epoch [3/15], Batch [200/1250], Loss: 0.7989\n",
      "Epoch [3/15], Batch [300/1250], Loss: 0.9612\n",
      "Epoch [3/15], Batch [400/1250], Loss: 0.8949\n",
      "Epoch [3/15], Batch [500/1250], Loss: 0.7071\n",
      "Epoch [3/15], Batch [600/1250], Loss: 0.9037\n",
      "Epoch [3/15], Batch [700/1250], Loss: 0.6907\n",
      "Epoch [3/15], Batch [800/1250], Loss: 0.6670\n",
      "Epoch [3/15], Batch [900/1250], Loss: 0.7419\n",
      "Epoch [3/15], Batch [1000/1250], Loss: 0.7154\n",
      "Epoch [3/15], Batch [1100/1250], Loss: 0.8778\n",
      "Epoch [3/15], Batch [1200/1250], Loss: 0.8248\n",
      "Epoch [4/15], Batch [100/1250], Loss: 0.7873\n",
      "Epoch [4/15], Batch [200/1250], Loss: 0.5839\n",
      "Epoch [4/15], Batch [300/1250], Loss: 0.5321\n",
      "Epoch [4/15], Batch [400/1250], Loss: 0.8042\n",
      "Epoch [4/15], Batch [500/1250], Loss: 0.6557\n",
      "Epoch [4/15], Batch [600/1250], Loss: 0.7489\n",
      "Epoch [4/15], Batch [700/1250], Loss: 0.6695\n",
      "Epoch [4/15], Batch [800/1250], Loss: 0.8435\n",
      "Epoch [4/15], Batch [900/1250], Loss: 0.5812\n",
      "Epoch [4/15], Batch [1000/1250], Loss: 0.6347\n",
      "Epoch [4/15], Batch [1100/1250], Loss: 0.5524\n",
      "Epoch [4/15], Batch [1200/1250], Loss: 0.5293\n",
      "Epoch [5/15], Batch [100/1250], Loss: 0.7280\n",
      "Epoch [5/15], Batch [200/1250], Loss: 0.5230\n",
      "Epoch [5/15], Batch [300/1250], Loss: 0.6415\n",
      "Epoch [5/15], Batch [400/1250], Loss: 0.8095\n",
      "Epoch [5/15], Batch [500/1250], Loss: 0.7986\n",
      "Epoch [5/15], Batch [600/1250], Loss: 0.6478\n",
      "Epoch [5/15], Batch [700/1250], Loss: 0.5888\n",
      "Epoch [5/15], Batch [800/1250], Loss: 0.6001\n",
      "Epoch [5/15], Batch [900/1250], Loss: 0.6337\n",
      "Epoch [5/15], Batch [1000/1250], Loss: 0.5875\n",
      "Epoch [5/15], Batch [1100/1250], Loss: 0.5821\n",
      "Epoch [5/15], Batch [1200/1250], Loss: 0.6411\n",
      "Epoch [6/15], Batch [100/1250], Loss: 0.4766\n",
      "Epoch [6/15], Batch [200/1250], Loss: 0.4617\n",
      "Epoch [6/15], Batch [300/1250], Loss: 0.6261\n",
      "Epoch [6/15], Batch [400/1250], Loss: 0.5825\n",
      "Epoch [6/15], Batch [500/1250], Loss: 0.5908\n",
      "Epoch [6/15], Batch [600/1250], Loss: 0.7705\n",
      "Epoch [6/15], Batch [700/1250], Loss: 0.4293\n",
      "Epoch [6/15], Batch [800/1250], Loss: 0.6092\n",
      "Epoch [6/15], Batch [900/1250], Loss: 0.6312\n",
      "Epoch [6/15], Batch [1000/1250], Loss: 0.4028\n",
      "Epoch [6/15], Batch [1100/1250], Loss: 0.3895\n",
      "Epoch [6/15], Batch [1200/1250], Loss: 0.6124\n",
      "Epoch [7/15], Batch [100/1250], Loss: 0.5495\n",
      "Epoch [7/15], Batch [200/1250], Loss: 0.5101\n",
      "Epoch [7/15], Batch [300/1250], Loss: 0.6844\n",
      "Epoch [7/15], Batch [400/1250], Loss: 0.4252\n",
      "Epoch [7/15], Batch [500/1250], Loss: 0.6696\n",
      "Epoch [7/15], Batch [600/1250], Loss: 0.5142\n",
      "Epoch [7/15], Batch [700/1250], Loss: 0.6004\n",
      "Epoch [7/15], Batch [800/1250], Loss: 0.5218\n",
      "Epoch [7/15], Batch [900/1250], Loss: 0.5330\n",
      "Epoch [7/15], Batch [1000/1250], Loss: 0.4380\n",
      "Epoch [7/15], Batch [1100/1250], Loss: 0.6370\n",
      "Epoch [7/15], Batch [1200/1250], Loss: 0.6266\n",
      "Epoch [8/15], Batch [100/1250], Loss: 0.5299\n",
      "Epoch [8/15], Batch [200/1250], Loss: 0.5041\n",
      "Epoch [8/15], Batch [300/1250], Loss: 0.4876\n",
      "Epoch [8/15], Batch [400/1250], Loss: 0.4851\n",
      "Epoch [8/15], Batch [500/1250], Loss: 0.3812\n",
      "Epoch [8/15], Batch [600/1250], Loss: 0.4497\n",
      "Epoch [8/15], Batch [700/1250], Loss: 0.4651\n",
      "Epoch [8/15], Batch [800/1250], Loss: 0.4840\n",
      "Epoch [8/15], Batch [900/1250], Loss: 0.5557\n",
      "Epoch [8/15], Batch [1000/1250], Loss: 0.4785\n",
      "Epoch [8/15], Batch [1100/1250], Loss: 0.8039\n",
      "Epoch [8/15], Batch [1200/1250], Loss: 0.8475\n",
      "Epoch [9/15], Batch [100/1250], Loss: 0.2937\n",
      "Epoch [9/15], Batch [200/1250], Loss: 0.4105\n",
      "Epoch [9/15], Batch [300/1250], Loss: 0.6162\n",
      "Epoch [9/15], Batch [400/1250], Loss: 0.4911\n",
      "Epoch [9/15], Batch [500/1250], Loss: 0.4409\n",
      "Epoch [9/15], Batch [600/1250], Loss: 0.4645\n",
      "Epoch [9/15], Batch [700/1250], Loss: 0.3476\n",
      "Epoch [9/15], Batch [800/1250], Loss: 0.4504\n",
      "Epoch [9/15], Batch [900/1250], Loss: 0.4454\n",
      "Epoch [9/15], Batch [1000/1250], Loss: 0.5593\n",
      "Epoch [9/15], Batch [1100/1250], Loss: 0.5419\n",
      "Epoch [9/15], Batch [1200/1250], Loss: 0.5236\n",
      "Epoch [10/15], Batch [100/1250], Loss: 0.5195\n",
      "Epoch [10/15], Batch [200/1250], Loss: 0.5662\n",
      "Epoch [10/15], Batch [300/1250], Loss: 0.4854\n",
      "Epoch [10/15], Batch [400/1250], Loss: 0.5144\n",
      "Epoch [10/15], Batch [500/1250], Loss: 0.5228\n",
      "Epoch [10/15], Batch [600/1250], Loss: 0.3842\n",
      "Epoch [10/15], Batch [700/1250], Loss: 0.3612\n",
      "Epoch [10/15], Batch [800/1250], Loss: 0.4702\n",
      "Epoch [10/15], Batch [900/1250], Loss: 0.4063\n",
      "Epoch [10/15], Batch [1000/1250], Loss: 0.5335\n",
      "Epoch [10/15], Batch [1100/1250], Loss: 0.4379\n",
      "Epoch [10/15], Batch [1200/1250], Loss: 0.4935\n",
      "Epoch [11/15], Batch [100/1250], Loss: 0.4683\n",
      "Epoch [11/15], Batch [200/1250], Loss: 0.3199\n",
      "Epoch [11/15], Batch [300/1250], Loss: 0.4085\n",
      "Epoch [11/15], Batch [400/1250], Loss: 0.4495\n",
      "Epoch [11/15], Batch [500/1250], Loss: 0.5585\n",
      "Epoch [11/15], Batch [600/1250], Loss: 0.4440\n",
      "Epoch [11/15], Batch [700/1250], Loss: 0.5432\n",
      "Epoch [11/15], Batch [800/1250], Loss: 0.6916\n",
      "Epoch [11/15], Batch [900/1250], Loss: 0.4828\n",
      "Epoch [11/15], Batch [1000/1250], Loss: 0.4073\n",
      "Epoch [11/15], Batch [1100/1250], Loss: 0.6681\n",
      "Epoch [11/15], Batch [1200/1250], Loss: 0.5194\n",
      "Epoch [12/15], Batch [100/1250], Loss: 0.5847\n",
      "Epoch [12/15], Batch [200/1250], Loss: 0.3675\n",
      "Epoch [12/15], Batch [300/1250], Loss: 0.3108\n",
      "Epoch [12/15], Batch [400/1250], Loss: 0.5321\n",
      "Epoch [12/15], Batch [500/1250], Loss: 0.4924\n",
      "Epoch [12/15], Batch [600/1250], Loss: 0.5280\n",
      "Epoch [12/15], Batch [700/1250], Loss: 0.5320\n",
      "Epoch [12/15], Batch [800/1250], Loss: 0.2851\n",
      "Epoch [12/15], Batch [900/1250], Loss: 0.3536\n",
      "Epoch [12/15], Batch [1000/1250], Loss: 0.3956\n",
      "Epoch [12/15], Batch [1100/1250], Loss: 0.4115\n",
      "Epoch [12/15], Batch [1200/1250], Loss: 0.5792\n",
      "Epoch [13/15], Batch [100/1250], Loss: 0.4331\n",
      "Epoch [13/15], Batch [200/1250], Loss: 0.4219\n",
      "Epoch [13/15], Batch [300/1250], Loss: 0.3411\n",
      "Epoch [13/15], Batch [400/1250], Loss: 0.4200\n",
      "Epoch [13/15], Batch [500/1250], Loss: 0.3946\n",
      "Epoch [13/15], Batch [600/1250], Loss: 0.4096\n",
      "Epoch [13/15], Batch [700/1250], Loss: 0.2781\n",
      "Epoch [13/15], Batch [800/1250], Loss: 0.4602\n",
      "Epoch [13/15], Batch [900/1250], Loss: 0.3280\n",
      "Epoch [13/15], Batch [1000/1250], Loss: 0.3453\n",
      "Epoch [13/15], Batch [1100/1250], Loss: 0.3562\n",
      "Epoch [13/15], Batch [1200/1250], Loss: 0.3089\n",
      "Epoch [14/15], Batch [100/1250], Loss: 0.3313\n",
      "Epoch [14/15], Batch [200/1250], Loss: 0.4836\n",
      "Epoch [14/15], Batch [300/1250], Loss: 0.4825\n",
      "Epoch [14/15], Batch [400/1250], Loss: 0.4120\n",
      "Epoch [14/15], Batch [500/1250], Loss: 0.2905\n",
      "Epoch [14/15], Batch [600/1250], Loss: 0.4483\n",
      "Epoch [14/15], Batch [700/1250], Loss: 0.2385\n",
      "Epoch [14/15], Batch [800/1250], Loss: 0.5027\n",
      "Epoch [14/15], Batch [900/1250], Loss: 0.4058\n",
      "Epoch [14/15], Batch [1000/1250], Loss: 0.4844\n",
      "Epoch [14/15], Batch [1100/1250], Loss: 0.4182\n",
      "Epoch [14/15], Batch [1200/1250], Loss: 0.6677\n",
      "Epoch [15/15], Batch [100/1250], Loss: 0.4320\n",
      "Epoch [15/15], Batch [200/1250], Loss: 0.6089\n",
      "Epoch [15/15], Batch [300/1250], Loss: 0.5653\n",
      "Epoch [15/15], Batch [400/1250], Loss: 0.4870\n",
      "Epoch [15/15], Batch [500/1250], Loss: 0.4891\n",
      "Epoch [15/15], Batch [600/1250], Loss: 0.4491\n",
      "Epoch [15/15], Batch [700/1250], Loss: 0.2822\n",
      "Epoch [15/15], Batch [800/1250], Loss: 0.3550\n",
      "Epoch [15/15], Batch [900/1250], Loss: 0.3703\n",
      "Epoch [15/15], Batch [1000/1250], Loss: 0.3170\n",
      "Epoch [15/15], Batch [1100/1250], Loss: 0.5122\n",
      "Epoch [15/15], Batch [1200/1250], Loss: 0.3106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.91      0.66      8871\n",
      "           1       0.82      0.30      0.44      6024\n",
      "           2       0.38      0.18      0.24      5067\n",
      "           3       0.25      0.17      0.20       554\n",
      "\n",
      "    accuracy                           0.53     20516\n",
      "   macro avg       0.49      0.39      0.38     20516\n",
      "weighted avg       0.56      0.53      0.48     20516\n",
      "\n",
      "Test Accuracy: 52.74%\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.lstm(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "              weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "              )\n",
    "        return hidden\n",
    "\n",
    "train_data = X_train3\n",
    "train_labels = y_train.to_numpy()\n",
    "test_data = X_test3\n",
    "test_labels = y_test.to_numpy()\n",
    "num_epochs = 40\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_data).float(), torch.from_numpy(train_labels).long())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)   \n",
    "\n",
    "LSTM_model = LSTMModel(300,40,4,1)\n",
    "optimizer = torch.optim.Adam(LSTM_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        batch_size = inputs.shape[0]\n",
    "        h = LSTM_model.init_hidden(batch_size)\n",
    "        h = tuple([e.data for e in h])\n",
    "        optimizer.zero_grad()\n",
    "        outputs,h = LSTM_model(inputs,h)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     LSTM_model.eval()\n",
    "#     test_data = torch.from_numpy(test_data).float()\n",
    "#     test_labels = torch.from_numpy(test_labels).long()\n",
    "#     batch_size = test_data.shape[0]\n",
    "#     seq_length = 1  # we are processing each sample as a single sequence\n",
    "#     input_dim = test_data.shape[1]\n",
    "#     test_data = test_data.view(batch_size, seq_length, input_dim)  # reshape the input tensor\n",
    "#     h = LSTM_model.init_hidden(batch_size)\n",
    "#     outputs, h = LSTM_model(test_data, h)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "#     total = test_labels.size(0)\n",
    "#     correct = (predicted == test_labels).sum().item()\n",
    "#     print(classification_report(predicted, test_labels))\n",
    "#     print('Test Accuracy: {:.2f}%'.format(100 * correct / total))\n",
    "#  Test the model\n",
    "with torch.no_grad():\n",
    "    LSTM_model.eval()\n",
    "    test_data = torch.from_numpy(test_data).float()\n",
    "    test_labels = torch.from_numpy(test_labels).long()\n",
    "    h = LSTM_model.init_hidden(test_data.shape[0])\n",
    "    outputs,h = LSTM_model(test_data,h)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = test_labels.size(0)\n",
    "    correct = (predicted == test_labels).sum().item()\n",
    "    print(classification_report(predicted,test_labels))\n",
    "\n",
    "    print('Test Accuracy: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.91      0.66      8871\n",
      "           1       0.82      0.30      0.44      6024\n",
      "           2       0.38      0.18      0.24      5067\n",
      "           3       0.25      0.17      0.20       554\n",
      "\n",
      "    accuracy                           0.53     20516\n",
      "   macro avg       0.49      0.39      0.38     20516\n",
      "weighted avg       0.56      0.53      0.48     20516\n",
      "\n",
      "Test Accuracy: 52.74%\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predicted,test_labels))\n",
    "\n",
    "print('Test Accuracy: {:.2f}%'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
